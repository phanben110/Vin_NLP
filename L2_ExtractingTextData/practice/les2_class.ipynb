{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpiOxnkRTig1"
   },
   "source": [
    "# Assignment 3: Trích xuất dữ liệu văn bản\n",
    "Tổng quan: \n",
    "- Trong bài này chúng ta sẽ lần lượt thực hành trích xuất dữ liệu văn bản từ các tài liệu pdf, docx, JSON\n",
    "- Sau đó ta sẽ làm sạch (clean) các text thu được sử dụng biểu thức chính quy\n",
    "- Bài tập yêu cầu các kiến thức về lập trình python với các thư viện: PyPDF2, docx, json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaTx47IRTig6"
   },
   "source": [
    "## Câu hỏi 1: trích xuất dữ liệu từ file pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PKq2X4hTTig7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Users/benphan/opt/anaconda3/envs/ai/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/benphan/opt/anaconda3/envs/ai/lib/python3.7/site-packages (from PyPDF2) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt thư viện PyPDF2\n",
    "!pip install PyPDF2  # bỏ dấu \"#\" đầu dòng để chạy cài đặt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xIQPzVQSTig7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import thư viện\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54II-VoTTig8"
   },
   "source": [
    "Hoàn thiện đoạn chương trình sau:\n",
    "- Đọc nội dung file và các thông tin về số trang của file \"NLP1.pdf\"\n",
    "- Lưu nội dung đã extract vào biến dạng string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các hàm trong thư viện PyPDF2:\n",
      "['DocumentInformation', 'PageObject', 'PageRange', 'PaperSize', 'PasswordType', 'PdfFileMerger', 'PdfFileReader', 'PdfFileWriter', 'PdfMerger', 'PdfReader', 'PdfWriter', 'Transformation', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_cmap', '_codecs', '_encryption', '_merger', '_page', '_reader', '_security', '_utils', '_version', '_writer', 'constants', 'errors', 'filters', 'generic', 'pagerange', 'papersizes', 'parse_filename_page_ranges', 'types', 'xmp']\n"
     ]
    }
   ],
   "source": [
    "print(\"Các hàm trong thư viện PyPDF2:\")\n",
    "print(dir(PyPDF2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Kw9SnuXOTig8",
    "outputId": "9e67462c-93b1-4ab0-e4ff-182b722d3d0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Content on page:1\n",
      "===================\n",
      "NLP là gì?  \n",
      "Lập trình ngôn ngữ tư duy NLP là một cách tiếp cận giả khoa học trong giao tiếp, phát triển cá nhân và tâm lý trị liệ\n",
      "u được tạo ra bởi Richard Bandler và John Gri ở California, Hoa Kỳ vào những năm 1970. Những người sáng tạo N\n",
      "LP tuyên bố rằng có một mối liên hệ giữa quá trình thần kinh (neuro), ngôn ngữ (linguistic) và các mô hình hành vi \n",
      "được học thông qua kinh nghiệm (programming) hay viết tắt gọi là ngôn ngữ NLP. Và những điều này có thể được t\n",
      "hay đổi để đạt được các mục tiêu cụ thể trong cuộc sống.  \n",
      "  \n",
      "NLP-voi-3-thanh-to-quan-trong-neuro-linguistic-programming  \n",
      "NLP với 3 thành tố quan trọng: neuro – linguistic – programming  \n",
      "Bandler và Gri nói rằng phương pháp NLP có thể “mô hình hóa” các kỹ năng của những người đặc biệt, cho phép bấ\n",
      "t cứ ai cũng có thể học được những kỹ năng đó. Họ cho biết thêm, NLP có thể điều trị các vấn đề như ám ảnh, trầm c\n",
      "ảm, hội chứng tic, bệnh tâm lý, dị ứng và rối loạn học tập.  \n",
      "  \n",
      "Hiểu một cách đơn giản, xử lý ngôn ngữ tự nhiên (NLP) cho phép bạn làm được những gì một thiên tài, một chuyên \n",
      "gia, một người thành công,… đã làm được bằng cách “lập trình” não bộ của mình theo những việc được cài đặt trong\n",
      " bộ não của họ.  \n",
      "  \n",
      "Nguồn gốc của NLP  \n",
      "Vào đầu những năm 1970, trong khi Tom Peters đang nghiên cứu môn sở trường học trong nhiều tổ chức và tìm kiế\n",
      "m những chiến lược thành công, thì John Grinder (nhà ngôn ngữ học) và Richard Bandler (nhà toán học) lại cố gắng \n",
      "tìm kiếm những công cụ xuất sắc trong việc phát triển tiềm năng bản thân. Và họ tạo ra phương pháp NLP chứa đựn\n",
      "g 3 thành tố có ảnh hưởng vô cùng lớn đến quá trình hình thành kinh nghiệm cá nhân của mỗi người.  \n",
      "  \n",
      "Hai-nha-sang-lap-NLP  \n",
      "Hai nhà sáng lập NLP  \n",
      "Những người sáng lập NLP cho biết, con người hiếm khi phản ứng trực tiếp với thế giới xung quanh. Ban đầu, họ dù\n",
      "ng những gì thu nhận được từ thế giới bên ngoài trong quá trình sống để “lập trình” cho bộ não của mình. Tức là bộ \n",
      "não sẽ tạo nên những mô thức hành vi và phản ứng trong các hoàn cảnh khác nhau của cuộc sống. Rồi họ hành động \n",
      "và phản ứng theo các lộ trình đã được cài đặt sẵn đó. Chẳng hạn, một số người luôn luôn hét lên khi nhìn thấy gián, \n",
      "một số người khác ngay lập tức dơ chân lên giẫm, một số người khác lại mặc kệ,…  \n",
      "  \n",
      "Từ quan điểm này, họ tạo ra NLP – phương pháp của họ có thể mô hình hoá các hoạt động phức tạp của con người v\n",
      "à cho người khác học hỏi.   \n",
      "  \n",
      "Theo Bandler và Gri, NLP bao gồm một phương pháp được gọi là mô hình hóa, cộng với một tập hợp các kỹ thuật b\n",
      "ắt nguồn nhiều công trình của Virginia Satir, Milton Erickson và Fritz Perls . Bandler và Gri cũng xây dựng NLP dự\n",
      "a trên lý thuyết của Gregory Bateson, Alfred Korzybski và Noam Chomsky (ngữ pháp chuyển đổi đặc biệt), cũng nh\n",
      "ư ý tưởng và kỹ thuật từ Carlos Castaneda.  \n",
      "  \n",
      "Gần đây (khoảng năm 1997), Bandler tuyên bố: “NLP được phát triển dựa trên việc tìm hiểu hoạt động và mô hình h\n",
      "oá nó. Để mô hình hoá các hành vi, tôi đã sử dụng mọi thứ từ ngôn ngữ học đến hình ba chiều,… Các mô hình hành \n",
      "vi được tạo nên đều là các mô hình chính thức dựa trên toán học, các nguyên tắc logic như tính toán vị ngữ và các ph\n",
      "ương trình toán học cơ bản của hình ba chiều”.  \n",
      "  \n",
      "Nguyên lý khoa học về NLP  \n",
      "Một trong những giáo sư nổi tiếng được trao giải Nobel về những công trình nghiên cứu của mình – Gerald Edelman\n",
      ", đã trải qua hơn 30 năm để tìm hiểu cách hoạt động của bộ não con người. Ông đã chứng minh được hơn 10 tỉ tế bà\n",
      "o thần kinh của mỗi con người đã sắp xếp thành từng nhóm và hình thành nên những biểu đồ phản ánh lại trải nghiệ\n",
      "m của chúng ta. Chúng cho phép con người ý thức được về thế giới xung quanh, đây chính là sự khác biệt của loài n\n",
      "gười so với những loài động vật còn lại. Sự liên kết giữa các tế bào não càng hoạt động nhiều thì càng phát triển theo\n",
      " tỉ lệ thuận, còn những tế bào còn lại sẽ chết dần.  \n",
      "  \n",
      "NLP sinh ra để nghiên cứu chi tiết về cách mỗi cá nhân hành xử theo thói quen của họ. Với Neuro – Linguistic – Pro\n",
      "gramming chúng ta có thể nắm bắt được những khuôn mẫu nào đang thực hiện những nhu cầu của bản thân sau đó t\n",
      "hêm vào dó những khuôn mẫu mới để đạt hiệu quả cao hơn cho bất kì trường hợp nào mà trước kia ta đã xử lý chưa \n",
      "===================\n",
      "Content on page:2\n",
      "===================\n",
      "tốt.  \n",
      "  \n",
      "Một trong những điều đặc biệt của NLP đó chính là: Ta có thể học được cách thức làm việc của một người và lặp lại \n",
      "nó, đó chính là nguyên lý mô phỏng của NLP (Principle of Modelling) – Một trong những phương pháp kinh điển tiế\n",
      "p cận cho việc phát hiện và bắt chước những kĩ năng của các nhân tài kiệt xuất trên thế giới.  \n",
      "  \n",
      "Để nhấn mạnh một điều rằng “Biết mình biết ta, trăm trận trăm thắng” – Khi hiểu bản thân mình, chúng ta có thể lựa\n",
      " chọn người mà ta sẽ trở thành trong tương lai.  \n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "pdfdoc = PyPDF2.PdfFileReader('/Users/benphan/vinbigdata/CTAI_VinBigdata/NLP/L2_ExtractingTextData/Lesson2/NLP1.pdf') \n",
    "for i in range(pdfdoc.numPages):\n",
    "    current_page = pdfdoc.getPage(i)\n",
    "    print(\"===================\")\n",
    "    print(\"Content on page:\" + str(i + 1))\n",
    "    print(\"===================\")\n",
    "    print(current_page.extractText())\n",
    "\n",
    "#### END YOUR CODE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud4Fb7UkTig9"
   },
   "source": [
    "## Câu hỏi 2: trích xuất dữ liệu văn bản từ file Word\n",
    "Hoàn thiện đoạn chương trình sau:\n",
    "- Trích xuất dữ liệu từ file \"NLP2.docx\" \n",
    "- Nội dung lưu vào một biến string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OrwOmaKCTig-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /Users/benphan/opt/anaconda3/envs/ai/lib/python3.7/site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /Users/benphan/opt/anaconda3/envs/ai/lib/python3.7/site-packages (from python-docx) (4.9.1)\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt thư viện docx\n",
    "!pip install python-docx  # bỏ dấu \"#\" đầu dòng để chạy cài đặt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EsFmb3KlTig-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<docx.text.paragraph.Paragraph object at 0x7f8054c13ed0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13590>, <docx.text.paragraph.Paragraph object at 0x7f8054c13a10>, <docx.text.paragraph.Paragraph object at 0x7f8054c13ad0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13d10>, <docx.text.paragraph.Paragraph object at 0x7f8054c13550>, <docx.text.paragraph.Paragraph object at 0x7f8054c13890>, <docx.text.paragraph.Paragraph object at 0x7f8054c13e50>, <docx.text.paragraph.Paragraph object at 0x7f8054c13b50>, <docx.text.paragraph.Paragraph object at 0x7f8054c13350>, <docx.text.paragraph.Paragraph object at 0x7f8054c13850>, <docx.text.paragraph.Paragraph object at 0x7f8054c13a50>, <docx.text.paragraph.Paragraph object at 0x7f8054c133d0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13bd0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13b90>, <docx.text.paragraph.Paragraph object at 0x7f8054c13d50>, <docx.text.paragraph.Paragraph object at 0x7f8054c13f50>, <docx.text.paragraph.Paragraph object at 0x7f8054c130d0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13410>, <docx.text.paragraph.Paragraph object at 0x7f8054c13710>, <docx.text.paragraph.Paragraph object at 0x7f8054c13cd0>, <docx.text.paragraph.Paragraph object at 0x7f8054c13290>, <docx.text.paragraph.Paragraph object at 0x7f8054c150d0>, <docx.text.paragraph.Paragraph object at 0x7f8054c15110>]\n"
     ]
    }
   ],
   "source": [
    "#Import library\n",
    "from docx import Document\n",
    "document = Document('/Users/benphan/vinbigdata/CTAI_VinBigdata/NLP/L2_ExtractingTextData/Lesson2/NLP2.docx')\n",
    "print(document.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p6RQGm5qTig_",
    "outputId": "e594472f-e305-4fde-f4da-26fc2bb26409"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jc/9krvg9jj60x85qvg498y04sh0000gn/T/ipykernel_7365/3767513801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### YOUR CODE HERE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "for p in document.paragraphs:\n",
    "    print(p.text)\n",
    "import os \n",
    "os.mkdir \n",
    "# print i love you \n",
    "print(\"I love you\")\n",
    "# create array image one\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "# create array image two\n",
    "\n",
    "\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tP_JGO_Tig_"
   },
   "source": [
    "## Câu hỏi 3: trích xuất văn bản từ file json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c756VteWTihA"
   },
   "outputs": [],
   "source": [
    "# import thư viện\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ltRJ6sCTihA"
   },
   "source": [
    "Hoàn thiện đoạn chương trình sau:\n",
    "- Đọc dữ liệu từ file \"NLP3.json\"\n",
    "- Sau đó nối các bản tin (News) lại, và lưu vào một biến string, mỗi bản tin trên một dòng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8v4ai8C4TihB",
    "outputId": "20467a0e-c280-4153-ce7b-8848e27c571a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news1': 'Xử lý ngôn ngữ tự nhiên (NLP) liên quan đến việc xây dựng các thuật toán tính toán để tự động phân tích và thể hiện ngôn ngữ của con người. Các hệ thống dựa trên NLP đã kích hoạt một loạt các ứng dụng như công cụ tìm kiếm mạnh mẽ của Google, và gần đây, trợ lý giọng nói của Amazon, có tên Alexa. NLP cũng trở nên hữu ích để dạy cho máy móc khả năng thực hiện các nhiệm vụ liên quan đến ngôn ngữ tự nhiên phức tạp như dịch máy và tạo hội thoại.',\n",
       " 'news2': 'Trong một thời gian dài, phần lớn các phương pháp được sử dụng để nghiên cứu các vấn đề NLP sử dụng các mô hình học máy nông và các tính năng thủ công, tốn thời gian. Điều này dẫn đến các vấn đề như lời nguyền về chiều hướng vì thông tin ngôn ngữ được thể hiện bằng các biểu hiện thưa thớt (tính năng chiều cao). Tuy nhiên, với sự phổ biến và thành công gần đây của các từ nhúng (biểu diễn phân tán, chiều thấp), các mô hình dựa trên thần kinh đã đạt được kết quả vượt trội trên các tác vụ liên quan đến ngôn ngữ khác nhau so với các mô hình học máy truyền thống như SVM hoặc hồi quy logistic.',\n",
       " 'news3': 'Từ nhúng (Word Embeddings): Các vectơ phân phối, còn được gọi là nhúng từ, dựa trên cái gọi là giả thuyết phân phối - các từ xuất hiện trong ngữ cảnh tương tự có ý nghĩa tương tự. Các từ nhúng được đào tạo trước về một nhiệm vụ trong đó mục tiêu là dự đoán một từ dựa trên ngữ cảnh của nó, thường sử dụng một mạng lưới thần kinh nông. Hình dưới đây minh họa một mô hình ngôn ngữ thần kinh được đề xuất bởi Bengio và các đồng nghiệp',\n",
       " 'news4': 'Word2vec: Khoảng năm 2013, Mikolav và cộng sự, đã đề xuất cả mô hình CBOW và Skip-gram. CBOW là một cách tiếp cận thần kinh để xây dựng các từ nhúng và mục tiêu là tính xác suất có điều kiện của một từ mục tiêu cho các từ ngữ cảnh trong một kích thước cửa sổ nhất định. Mặt khác, Skip-gram là một cách tiếp cận thần kinh để xây dựng các từ nhúng, trong đó mục tiêu là dự đoán các từ ngữ cảnh xung quanh (nghĩa là xác suất có điều kiện) được đưa ra một từ mục tiêu trung tâm. Đối với cả hai mô hình, kích thước nhúng từ được xác định bằng tính toán (theo cách không giám sát) độ chính xác của dự đoán.',\n",
       " 'news5': 'CNN về cơ bản là một cách tiếp cận dựa trên thần kinh đại diện cho một chức năng tính năng được áp dụng để cấu thành các từ hoặc mô hình ngôn ngữ (n-gram) để trích xuất các tính năng cấp cao hơn. Các tính năng trừu tượng kết quả đã được sử dụng hiệu quả để phân tích tình cảm, dịch máy và trả lời câu hỏi, trong số các nhiệm vụ khác. Collobert và Weston là một trong những nhà nghiên cứu đầu tiên áp dụng các khung dựa trên CNN cho các nhiệm vụ NLP. Mục tiêu của phương pháp của họ là biến đổi các từ thành biểu diễn vectơ thông qua bảng tra cứu, dẫn đến cách tiếp cận nhúng từ nguyên thủy để học các trọng số trong quá trình đào tạo mạng (xem hình bên dưới).',\n",
       " 'news6': 'RNN là các phương pháp dựa trên nơ-ron chuyên biệt có hiệu quả trong việc xử lý thông tin tuần tự. Một RNN áp dụng đệ quy một tính toán cho mọi trường hợp của chuỗi đầu vào có điều kiện dựa trên các kết quả được tính toán trước đó. Các chuỗi này thường được biểu thị bằng một vectơ kích thước cố định được cung cấp tuần tự (từng cái một) cho một đơn vị định kỳ'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('/Users/benphan/vinbigdata/CTAI_VinBigdata/NLP/L2_ExtractingTextData/Lesson2/NLP3.json')\n",
    "dataJson = dict( json.load(f) )\n",
    "dataJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xử lý ngôn ngữ tự nhiên (NLP) liên quan đến việc xây dựng các thuật toán tính toán để tự động phân tích và thể hiện ngôn ngữ của con người. Các hệ thống dựa trên NLP đã kích hoạt một loạt các ứng dụng như công cụ tìm kiếm mạnh mẽ của Google, và gần đây, trợ lý giọng nói của Amazon, có tên Alexa. NLP cũng trở nên hữu ích để dạy cho máy móc khả năng thực hiện các nhiệm vụ liên quan đến ngôn ngữ tự nhiên phức tạp như dịch máy và tạo hội thoại.\n",
      "Trong một thời gian dài, phần lớn các phương pháp được sử dụng để nghiên cứu các vấn đề NLP sử dụng các mô hình học máy nông và các tính năng thủ công, tốn thời gian. Điều này dẫn đến các vấn đề như lời nguyền về chiều hướng vì thông tin ngôn ngữ được thể hiện bằng các biểu hiện thưa thớt (tính năng chiều cao). Tuy nhiên, với sự phổ biến và thành công gần đây của các từ nhúng (biểu diễn phân tán, chiều thấp), các mô hình dựa trên thần kinh đã đạt được kết quả vượt trội trên các tác vụ liên quan đến ngôn ngữ khác nhau so với các mô hình học máy truyền thống như SVM hoặc hồi quy logistic.\n",
      "Từ nhúng (Word Embeddings): Các vectơ phân phối, còn được gọi là nhúng từ, dựa trên cái gọi là giả thuyết phân phối - các từ xuất hiện trong ngữ cảnh tương tự có ý nghĩa tương tự. Các từ nhúng được đào tạo trước về một nhiệm vụ trong đó mục tiêu là dự đoán một từ dựa trên ngữ cảnh của nó, thường sử dụng một mạng lưới thần kinh nông. Hình dưới đây minh họa một mô hình ngôn ngữ thần kinh được đề xuất bởi Bengio và các đồng nghiệp\n",
      "Word2vec: Khoảng năm 2013, Mikolav và cộng sự, đã đề xuất cả mô hình CBOW và Skip-gram. CBOW là một cách tiếp cận thần kinh để xây dựng các từ nhúng và mục tiêu là tính xác suất có điều kiện của một từ mục tiêu cho các từ ngữ cảnh trong một kích thước cửa sổ nhất định. Mặt khác, Skip-gram là một cách tiếp cận thần kinh để xây dựng các từ nhúng, trong đó mục tiêu là dự đoán các từ ngữ cảnh xung quanh (nghĩa là xác suất có điều kiện) được đưa ra một từ mục tiêu trung tâm. Đối với cả hai mô hình, kích thước nhúng từ được xác định bằng tính toán (theo cách không giám sát) độ chính xác của dự đoán.\n",
      "CNN về cơ bản là một cách tiếp cận dựa trên thần kinh đại diện cho một chức năng tính năng được áp dụng để cấu thành các từ hoặc mô hình ngôn ngữ (n-gram) để trích xuất các tính năng cấp cao hơn. Các tính năng trừu tượng kết quả đã được sử dụng hiệu quả để phân tích tình cảm, dịch máy và trả lời câu hỏi, trong số các nhiệm vụ khác. Collobert và Weston là một trong những nhà nghiên cứu đầu tiên áp dụng các khung dựa trên CNN cho các nhiệm vụ NLP. Mục tiêu của phương pháp của họ là biến đổi các từ thành biểu diễn vectơ thông qua bảng tra cứu, dẫn đến cách tiếp cận nhúng từ nguyên thủy để học các trọng số trong quá trình đào tạo mạng (xem hình bên dưới).\n",
      "RNN là các phương pháp dựa trên nơ-ron chuyên biệt có hiệu quả trong việc xử lý thông tin tuần tự. Một RNN áp dụng đệ quy một tính toán cho mọi trường hợp của chuỗi đầu vào có điều kiện dựa trên các kết quả được tính toán trước đó. Các chuỗi này thường được biểu thị bằng một vectơ kích thước cố định được cung cấp tuần tự (từng cái một) cho một đơn vị định kỳ\n"
     ]
    }
   ],
   "source": [
    "dataJ = ''\n",
    "for data in dataJson.values(): \n",
    "   print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGIbdK_ETihB"
   },
   "source": [
    "## Câu hỏi 4: Xử lý dữ liệu đã trích chọn được"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyKcEoyfTihC"
   },
   "source": [
    "### Câu hỏi 4.1: từ các dữ liệu trích chọn từ các câu hỏi 1, 2, 3. Hãy nối chúng lại với nhau vào một biến string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DWxF48nrTihC",
    "outputId": "11677d8d-3bcc-443d-dfb9-c435ab47710e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP là gì?  \n",
      "Lập trình ngôn ngữ tư duy NLP là một cách tiếp cận giả khoa học trong giao tiếp, phát triển cá nhân và tâm lý trị liệ\n",
      "u được tạo ra bởi Richard Bandler và John Gri ở California, Hoa Kỳ vào những năm 1970. Những người sáng tạo N\n",
      "LP tuyên bố rằng có một mối liên hệ giữa quá trình thần kinh (neuro), ngôn ngữ (linguistic) và các mô hình hành vi \n",
      "được học thông qua kinh nghiệm (programming) hay viết tắt gọi là ngôn ngữ NLP. Và những điều này có thể được t\n",
      "hay đổi để đạt được các mục tiêu cụ thể trong cuộc sống.  \n",
      "  \n",
      "NLP-voi-3-thanh-to-quan-trong-neuro-linguistic-programming  \n",
      "NLP với 3 thành tố quan trọng: neuro – linguistic – programming  \n",
      "Bandler và Gri nói rằng phương pháp NLP có thể “mô hình hóa” các kỹ năng của những người đặc biệt, cho phép bấ\n",
      "t cứ ai cũng có thể học được những kỹ năng đó. Họ cho biết thêm, NLP có thể điều trị các vấn đề như ám ảnh, trầm c\n",
      "ảm, hội chứng tic, bệnh tâm lý, dị ứng và rối loạn học tập.  \n",
      "  \n",
      "Hiểu một cách đơn giản, xử lý ngôn ngữ tự nhiên (NLP) cho phép bạn làm được những gì một thiên tài, một chuyên \n",
      "gia, một người thành công,… đã làm được bằng cách “lập trình” não bộ của mình theo những việc được cài đặt trong\n",
      " bộ não của họ.  \n",
      "  \n",
      "Nguồn gốc của NLP  \n",
      "Vào đầu những năm 1970, trong khi Tom Peters đang nghiên cứu môn sở trường học trong nhiều tổ chức và tìm kiế\n",
      "m những chiến lược thành công, thì John Grinder (nhà ngôn ngữ học) và Richard Bandler (nhà toán học) lại cố gắng \n",
      "tìm kiếm những công cụ xuất sắc trong việc phát triển tiềm năng bản thân. Và họ tạo ra phương pháp NLP chứa đựn\n",
      "g 3 thành tố có ảnh hưởng vô cùng lớn đến quá trình hình thành kinh nghiệm cá nhân của mỗi người.  \n",
      "  \n",
      "Hai-nha-sang-lap-NLP  \n",
      "Hai nhà sáng lập NLP  \n",
      "Những người sáng lập NLP cho biết, con người hiếm khi phản ứng trực tiếp với thế giới xung quanh. Ban đầu, họ dù\n",
      "ng những gì thu nhận được từ thế giới bên ngoài trong quá trình sống để “lập trình” cho bộ não của mình. Tức là bộ \n",
      "não sẽ tạo nên những mô thức hành vi và phản ứng trong các hoàn cảnh khác nhau của cuộc sống. Rồi họ hành động \n",
      "và phản ứng theo các lộ trình đã được cài đặt sẵn đó. Chẳng hạn, một số người luôn luôn hét lên khi nhìn thấy gián, \n",
      "một số người khác ngay lập tức dơ chân lên giẫm, một số người khác lại mặc kệ,…  \n",
      "  \n",
      "Từ quan điểm này, họ tạo ra NLP – phương pháp của họ có thể mô hình hoá các hoạt động phức tạp của con người v\n",
      "à cho người khác học hỏi.   \n",
      "  \n",
      "Theo Bandler và Gri, NLP bao gồm một phương pháp được gọi là mô hình hóa, cộng với một tập hợp các kỹ thuật b\n",
      "ắt nguồn nhiều công trình của Virginia Satir, Milton Erickson và Fritz Perls . Bandler và Gri cũng xây dựng NLP dự\n",
      "a trên lý thuyết của Gregory Bateson, Alfred Korzybski và Noam Chomsky (ngữ pháp chuyển đổi đặc biệt), cũng nh\n",
      "ư ý tưởng và kỹ thuật từ Carlos Castaneda.  \n",
      "  \n",
      "Gần đây (khoảng năm 1997), Bandler tuyên bố: “NLP được phát triển dựa trên việc tìm hiểu hoạt động và mô hình h\n",
      "oá nó. Để mô hình hoá các hành vi, tôi đã sử dụng mọi thứ từ ngôn ngữ học đến hình ba chiều,… Các mô hình hành \n",
      "vi được tạo nên đều là các mô hình chính thức dựa trên toán học, các nguyên tắc logic như tính toán vị ngữ và các ph\n",
      "ương trình toán học cơ bản của hình ba chiều”.  \n",
      "  \n",
      "Nguyên lý khoa học về NLP  \n",
      "Một trong những giáo sư nổi tiếng được trao giải Nobel về những công trình nghiên cứu của mình – Gerald Edelman\n",
      ", đã trải qua hơn 30 năm để tìm hiểu cách hoạt động của bộ não con người. Ông đã chứng minh được hơn 10 tỉ tế bà\n",
      "o thần kinh của mỗi con người đã sắp xếp thành từng nhóm và hình thành nên những biểu đồ phản ánh lại trải nghiệ\n",
      "m của chúng ta. Chúng cho phép con người ý thức được về thế giới xung quanh, đây chính là sự khác biệt của loài n\n",
      "gười so với những loài động vật còn lại. Sự liên kết giữa các tế bào não càng hoạt động nhiều thì càng phát triển theo\n",
      " tỉ lệ thuận, còn những tế bào còn lại sẽ chết dần.  \n",
      "  \n",
      "NLP sinh ra để nghiên cứu chi tiết về cách mỗi cá nhân hành xử theo thói quen của họ. Với Neuro – Linguistic – Pro\n",
      "gramming chúng ta có thể nắm bắt được những khuôn mẫu nào đang thực hiện những nhu cầu của bản thân sau đó t\n",
      "hêm vào dó những khuôn mẫu mới để đạt hiệu quả cao hơn cho bất kì trường hợp nào mà trước kia ta đã xử lý chưa tốt.  \n",
      "  \n",
      "Một trong những điều đặc biệt của NLP đó chính là: Ta có thể học được cách thức làm việc của một người và lặp lại \n",
      "nó, đó chính là nguyên lý mô phỏng của NLP (Principle of Modelling) – Một trong những phương pháp kinh điển tiế\n",
      "p cận cho việc phát hiện và bắt chước những kĩ năng của các nhân tài kiệt xuất trên thế giới.  \n",
      "  \n",
      "Để nhấn mạnh một điều rằng “Biết mình biết ta, trăm trận trăm thắng” – Khi hiểu bản thân mình, chúng ta có thể lựa\n",
      " chọn người mà ta sẽ trở thành trong tương lai.  \n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "pdfdoc = PyPDF2.PdfFileReader('/Users/benphan/vinbigdata/CTAI_VinBigdata/NLP/L2_ExtractingTextData/Lesson2/NLP1.pdf') \n",
    "strPage = ''\n",
    "for i in range(pdfdoc.numPages):\n",
    "    current_page = pdfdoc.getPage(i)\n",
    "    #print(\"===================\")\n",
    "    #print(\"Content on page:\" + str(i + 1))\n",
    "    #print(\"===================\")\n",
    "    strPage += current_page.extractText()\n",
    "    #print(current_page.extractText())\n",
    "print(strPage)\n",
    "\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Token hoá và phân loại văn bản (Tokenization and Text Classification)Token hoá (Tokenization) là quá trình biến mọi thứ thành tài sản kỹ thuật số, bao gồm chia nhỏ các từ thành từng phần (hoặc mã token) mà máy móc có thể hiểu được. Các tài liệu, văn bản bằng tiếng Anh rất dễ token hóa vì chúng có khoảng cách rõ ràng (clear spaces) giữa các từ và đoạn văn. Tuy nhiên, hầu hết các ngôn ngữ khác đều là những thử thách hoàn toàn mới. Ví dụ, các ngôn ngữ logic như tiếng Quảng Đông, tiếng Quan Thoại và tiếng Kanji của Nhật Bản có thể là những thách thức, khó khăn vì chúng không có khoảng cách giữa các từ hoặc thậm chí là câu.Nhưng tất cả các ngôn ngữ tuân theo các quy tắc và mẫu nhất định. Thông qua học tập sâu, chúng ta có thể đào tạo các mô hình để thực hiện token hoá. Do đó, hầu hết các khóa học về AI và DL khuyến khích các chuyên gia DL thử nghiệm các mô hình đào tạo DL để xác định và hiểu các mẫu và văn bản này.Ngoài ra, các mô hình DL có thể phân loại và dự đoán chủ đề của văn bản, đoạn văn đó đang hướng tới. Ví dụ, mạng nơ ron tích chập CNN (Convolutional Neural Networks) và mạng nơ ron hồi quy RNN (Recurrent Neural Networks) có thể tự động phân loại tone và sắc thái của văn bản bằng cách sử dụng word embeddings giúp tìm ra mô hình không gian vector cho các từ. Hầu hết các nền tảng social media đều triển khai các hệ thống phân tích dựa trên CNN và RNN để gắn cờ và xác định nội dung spam trên nền tảng của họ. Phân loại văn bản cũng được áp dụng trong tìm kiếm trên web, nhận dạng ngôn ngữ và đánh giá khả năng đọc.2. Tạo chú thích cho hình ảnh (Generating Captions for Images)Tự động mô tả nội dung của một hình ảnh bằng cách sử dụng các câu tự nhiên là một nhiệm vụ đầy thách thức. Chú thích của hình ảnh không chỉ để nhận ra các đối tượng trong ảnh mà còn thể hiện cách chúng có liên quan tới nhau hay không cùng với các thuộc tính của chúng (mô hình nhận dạng hình ảnh). Ngoài mô hình nhận dạng ảnh ra, kiến thức ngữ nghĩa phải được thể hiện bằng ngôn ngữ tự nhiên cũng đòi hỏi một mô hình ngôn ngữ nữa.Căn chỉnh các yếu tố hình ảnh và ngữ nghĩa là cốt lõi để tạo chú thích hình ảnh một cách hoàn hảo. Các mô hình DL có thể giúp tự động mô tả nội dung của hình ảnh bằng cách sử dụng các câu tiếng Anh chính xác. Điều này có thể giúp những người khiếm thị dễ dàng truy cập nội dung trực tuyến.Trình tạo chú thích hình ảnh nơ ron của Google (NIC) dựa trên một network bao gồm CNN (ứng dụng trong Computer Vision), theo sao là RNN (ứng dụng trong xử lý ngôn ngữ) . Mô hình này sẽ tự động xem hình ảnh và tạo bản mô tả bằng tiếng Anh.3. Nhận dạng giọng nói (Speech Recognition)DL đang ngày càng được sử dụng để xây dựng và huấn luyện các mạng thần kinh để chuyển dịch các đầu vào là âm thanh (audio inputs) và thực hiện các nhiệm vụ nhận dạng và tách từ vựng phức tạp. Trên thực tế, các mô hình và phương pháp này được sử dụng trong xử lý tín hiệu, ngữ âm và nhận dạng từ, các lĩnh vực cốt lõi của nhận dạng giọng nói.Ví dụ, các mô hình DL có thể được đào tạo để xác định từng giọng nói cho người nói tương ứng và trả lời riêng từng người nói. Hơn nữa, các hệ thống nhận dạng giọng nói dựa trên CNN có thể dịch lời nói “raw” thành tin nhắn văn bản, điều này giúo cung cấp những insight thú vị liên quan đến người nói.4. Dịch máy (Machine Translation)Dịch máy (MT) là một nhiệm vụ cốt lõi trong xử lý ngôn ngữ tự nhiên, có thể điều tra việc sử dụng máy tính để dịch ngôn ngữ mà không cần sự can thiệp của con người. Gần đây, chỉ có các mô hình học sâu mới được sử dụng cho dịch máy bằng nơ ron. Không giống như dịch máy truyền thống, các mạng nơ ron sâu (DNN) cung cấp bản dịch chính xác và hiệu suất tốt hơn. Mạng nơ ron tích chập (RNN), mạng nơ ron truyền ngược (FNN), bộ mã hóa tự động đệ quy (RAE) và bộ nhớ dài ngắn hạn (LSTM) được sử dụng để huấn luyện máy chuyển đổi câu từ ngôn ngữ gốc của văn bản đó sang ngôn ngữ muốn chuyển đổi một cách chính xác.Các giải pháp DNN phù hợp được sử dụng cho các quy trình, chẳng hạn như căn chỉnh từ, quy tắc sắp xếp lại câu, xây dựng mô hình ngôn ngữ và tham gia dự đoán dịch để dịch câu mà không cần sử dụng một lượng lớn cơ sở dữ liệu các nguyên tắc.5. Trả lời câu hỏi (Question Answering)Các hệ thống QA này cố gắng trả lời một thắc mắc được đặt dưới dạng câu hỏi. Vì vậy, câu hỏi định nghĩa, câu hỏi tiểu sử và câu hỏi đa ngôn ngữ trong số các loại câu hỏi khác mà được hỏi bằng ngôn ngữ tự nhiên thì được trả lời bởi các hệ thống như vậy.Tạo ra một hệ thống trả lời câu hỏi đầy đủ chức năng là một trong những thách thức phổ biến mà các nhà nghiên cứu phải đối mặt trong phân khúc DL. Mặc dù các thuật toán học sâu đã đạt được tiến bộ đáng kể trong phân loại văn bản và hình ảnh trong quá khứ, nhưng không thể giải quyết các tác vụ liên quan đến lý luận logic (như vấn đề trả lời câu hỏi). Tuy nhiên, trong thời gian gần đây, các mô hình học sâu đang cải thiện hiệu suất và độ chính xác của các hệ thống QA này.Ví dụ, các mô hình mạng nơ ron tích chập có thể trả lời một cách chính xác các câu hỏi dài trong khi đó các cách tiếp cận truyền thống hồi xưa đã từng thất bại. Quan trọng hơn, mô hình DL được đào tạo theo cách mà không cần phải xây dựng hệ thống bằng kiến ​​thức ngôn ngữ như tạo một trình phân tách ngữ nghĩa6. Tóm tắt văn bản (Document Summarization)Việc tóm tắt văn bản đang đóng vai trò cực kỳ quan trọng khi ngày nay càng khối lượng dữ liệu (data) ngày càng gia tăng. Những tiến bộ mới nhất trong các mô hình sequence-to-sequence đã giúp các chuyên gia DL dễ dàng phát triển các mô hình tóm tắt văn bản tốt hơn. Hai loại tóm tắt văn bản cụ thể là Tóm tắt rút trích (Extract): là một bản tóm tắt  bao gồm các nội dung được rút trích từ văn bản gốc.Tóm tắt tóm lược (Abstract): là một bản tóm tắt có chứa các nội dung không được thể hiện trong văn bản gốc. Trong mô hình sequence-to-sequence (seq2seq), kỹ thuật attention là một kỹ thuật cho phép có thể học hiệu quả mô hình sinh cho những chuỗi có độ dài lớn và đã dành được sự quan tâm lớn của cộng đồng nghiên cứu.. Tham khảo sơ đồ bên dưới từ blog Pointer Generator của Abigail See.Mô hình seq2seq cơ bản bao gồm hai mạng neural thành phần được gọi là mạng mã hóa (encoder) RNN và mạng giải mã (decoder) RNN để sinh ra chuỗi đầu ra t1:m từ một chuỗi đầu vào x1:n. Mạng neural encoder mã hóa chuỗi đầu vào thành một vector c có độ dài cố định. Mạng neural decoder sẽ lần lượt sinh từng từ trong chuỗi đầu ra dựa trên vector c và những từ được dự đoán trước đó cho tới khi gặp từ kết thúc câu. Bởi vì mạng giải mã decoder có thể tự do tạo ra các từ theo bất kỳ thứ tự nào, nên mô hình seq2seq là một giải pháp vững mạnh cho tóm tắt tóm lược văn bản.\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "strDoc = ''\n",
    "for p in document.paragraphs:\n",
    "    #print(p.text)\n",
    "    strDoc += p.text\n",
    "print(strDoc)\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xử lý ngôn ngữ tự nhiên (NLP) liên quan đến việc xây dựng các thuật toán tính toán để tự động phân tích và thể hiện ngôn ngữ của con người. Các hệ thống dựa trên NLP đã kích hoạt một loạt các ứng dụng như công cụ tìm kiếm mạnh mẽ của Google, và gần đây, trợ lý giọng nói của Amazon, có tên Alexa. NLP cũng trở nên hữu ích để dạy cho máy móc khả năng thực hiện các nhiệm vụ liên quan đến ngôn ngữ tự nhiên phức tạp như dịch máy và tạo hội thoại.Trong một thời gian dài, phần lớn các phương pháp được sử dụng để nghiên cứu các vấn đề NLP sử dụng các mô hình học máy nông và các tính năng thủ công, tốn thời gian. Điều này dẫn đến các vấn đề như lời nguyền về chiều hướng vì thông tin ngôn ngữ được thể hiện bằng các biểu hiện thưa thớt (tính năng chiều cao). Tuy nhiên, với sự phổ biến và thành công gần đây của các từ nhúng (biểu diễn phân tán, chiều thấp), các mô hình dựa trên thần kinh đã đạt được kết quả vượt trội trên các tác vụ liên quan đến ngôn ngữ khác nhau so với các mô hình học máy truyền thống như SVM hoặc hồi quy logistic.Từ nhúng (Word Embeddings): Các vectơ phân phối, còn được gọi là nhúng từ, dựa trên cái gọi là giả thuyết phân phối - các từ xuất hiện trong ngữ cảnh tương tự có ý nghĩa tương tự. Các từ nhúng được đào tạo trước về một nhiệm vụ trong đó mục tiêu là dự đoán một từ dựa trên ngữ cảnh của nó, thường sử dụng một mạng lưới thần kinh nông. Hình dưới đây minh họa một mô hình ngôn ngữ thần kinh được đề xuất bởi Bengio và các đồng nghiệpWord2vec: Khoảng năm 2013, Mikolav và cộng sự, đã đề xuất cả mô hình CBOW và Skip-gram. CBOW là một cách tiếp cận thần kinh để xây dựng các từ nhúng và mục tiêu là tính xác suất có điều kiện của một từ mục tiêu cho các từ ngữ cảnh trong một kích thước cửa sổ nhất định. Mặt khác, Skip-gram là một cách tiếp cận thần kinh để xây dựng các từ nhúng, trong đó mục tiêu là dự đoán các từ ngữ cảnh xung quanh (nghĩa là xác suất có điều kiện) được đưa ra một từ mục tiêu trung tâm. Đối với cả hai mô hình, kích thước nhúng từ được xác định bằng tính toán (theo cách không giám sát) độ chính xác của dự đoán.CNN về cơ bản là một cách tiếp cận dựa trên thần kinh đại diện cho một chức năng tính năng được áp dụng để cấu thành các từ hoặc mô hình ngôn ngữ (n-gram) để trích xuất các tính năng cấp cao hơn. Các tính năng trừu tượng kết quả đã được sử dụng hiệu quả để phân tích tình cảm, dịch máy và trả lời câu hỏi, trong số các nhiệm vụ khác. Collobert và Weston là một trong những nhà nghiên cứu đầu tiên áp dụng các khung dựa trên CNN cho các nhiệm vụ NLP. Mục tiêu của phương pháp của họ là biến đổi các từ thành biểu diễn vectơ thông qua bảng tra cứu, dẫn đến cách tiếp cận nhúng từ nguyên thủy để học các trọng số trong quá trình đào tạo mạng (xem hình bên dưới).RNN là các phương pháp dựa trên nơ-ron chuyên biệt có hiệu quả trong việc xử lý thông tin tuần tự. Một RNN áp dụng đệ quy một tính toán cho mọi trường hợp của chuỗi đầu vào có điều kiện dựa trên các kết quả được tính toán trước đó. Các chuỗi này thường được biểu thị bằng một vectơ kích thước cố định được cung cấp tuần tự (từng cái một) cho một đơn vị định kỳ\n"
     ]
    }
   ],
   "source": [
    "dataJ = ''\n",
    "for data in dataJson.values():\n",
    "   dataJ += data\n",
    "print(dataJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RpOhWr8TihD"
   },
   "source": [
    "### Câu hỏi 4.2 Hoàn thiện hàm xử lý chuỗi bản tin\n",
    "Mô tả hàm: hàm này nhận tham số đầu vào là một chuỗi, và trả ra chuỗi đã được xử lý, công việc chính trong hàm như sau:\n",
    "- Thay thế \"\\…\" thành \" \". Gợi ý: dấu copy dấu ... ở đây '…' hoặc có thể lấy từ chính dữ liệu\n",
    "- Thay thế \"\\'ll\" thành \" \\'ll\"\n",
    "- Thay thế \",\" thành \" , \"\n",
    "- Thay thế \"!\" thành \" ! \"\n",
    "- Thay thế \"\\(\" thành \" \\( \"\n",
    "- Thay thế \"\\)\" thành \" \\) \"\n",
    "- Thay thế \"\\?\" thành \" \\? \"\n",
    "- Thay thế \"\\s{2,}\" thành \" \"\n",
    "- Cắt bỏ các khoảng trắng ở đầu\n",
    "- Chuyển văn bản thành chữ thường\n",
    "\n",
    "Gợi ý: sử dụng re.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "PX-FNkUxTihD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xử lý ngôn ngữ tự nhiên (NLP) liên quan đến việc xây dựng các thuật toán tính toán để tự động phân tích và thể hiện ngôn ngữ của con ngườiác hệ thống dựa trên NLP đã kích hoạt một loạt các ứng dụng như công cụ tìm kiếm mạnh mẽ của Google, và gần đây, trợ lý giọng nói của Amazon, có tên AlexaLP cũng trở nên hữu ích để dạy cho máy móc khả năng thực hiện các nhiệm vụ liên quan đến ngôn ngữ tự nhiên phức tạp như dịch máy và tạo hội thoạiong một thời gian dài, phần lớn các phương pháp được sử dụng để nghiên cứu các vấn đề NLP sử dụng các mô hình học máy nông và các tính năng thủ công, tốn thời gianiều này dẫn đến các vấn đề như lời nguyền về chiều hướng vì thông tin ngôn ngữ được thể hiện bằng các biểu hiện thưa thớt (tính năng chiều cao)uy nhiên, với sự phổ biến và thành công gần đây của các từ nhúng (biểu diễn phân tán, chiều thấp), các mô hình dựa trên thần kinh đã đạt được kết quả vượt trội trên các tác vụ liên quan đến ngôn ngữ khác nhau so với các mô hình học máy truyền thống như SVM hoặc hồi quy logistic nhúng (Word Embeddings): Các vectơ phân phối, còn được gọi là nhúng từ, dựa trên cái gọi là giả thuyết phân phối - các từ xuất hiện trong ngữ cảnh tương tự có ý nghĩa tương tựác từ nhúng được đào tạo trước về một nhiệm vụ trong đó mục tiêu là dự đoán một từ dựa trên ngữ cảnh của nó, thường sử dụng một mạng lưới thần kinh nôngình dưới đây minh họa một mô hình ngôn ngữ thần kinh được đề xuất bởi Bengio và các đồng nghiệpWord2vec: Khoảng năm 2013, Mikolav và cộng sự, đã đề xuất cả mô hình CBOW và Skip-gramBOW là một cách tiếp cận thần kinh để xây dựng các từ nhúng và mục tiêu là tính xác suất có điều kiện của một từ mục tiêu cho các từ ngữ cảnh trong một kích thước cửa sổ nhất địnhặt khác, Skip-gram là một cách tiếp cận thần kinh để xây dựng các từ nhúng, trong đó mục tiêu là dự đoán các từ ngữ cảnh xung quanh (nghĩa là xác suất có điều kiện) được đưa ra một từ mục tiêu trung tâmối với cả hai mô hình, kích thước nhúng từ được xác định bằng tính toán (theo cách không giám sát) độ chính xác của dự đoánN về cơ bản là một cách tiếp cận dựa trên thần kinh đại diện cho một chức năng tính năng được áp dụng để cấu thành các từ hoặc mô hình ngôn ngữ (n-gram) để trích xuất các tính năng cấp cao hơnác tính năng trừu tượng kết quả đã được sử dụng hiệu quả để phân tích tình cảm, dịch máy và trả lời câu hỏi, trong số các nhiệm vụ khácollobert và Weston là một trong những nhà nghiên cứu đầu tiên áp dụng các khung dựa trên CNN cho các nhiệm vụ NLPục tiêu của phương pháp của họ là biến đổi các từ thành biểu diễn vectơ thông qua bảng tra cứu, dẫn đến cách tiếp cận nhúng từ nguyên thủy để học các trọng số trong quá trình đào tạo mạng (xem hình bên dưới)N là các phương pháp dựa trên nơ-ron chuyên biệt có hiệu quả trong việc xử lý thông tin tuần tựột RNN áp dụng đệ quy một tính toán cho mọi trường hợp của chuỗi đầu vào có điều kiện dựa trên các kết quả được tính toán trước đóác chuỗi này thường được biểu thị bằng một vectơ kích thước cố định được cung cấp tuần tự (từng cái một) cho một đơn vị định kỳ'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import thư viện regular expression\n",
    "import re \n",
    "re.sub(\"\\...\",\"\",dataJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "OPguKzqCTihD"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "#### YOUR CODE HERE ####\n",
    "   string = string.replace(\"\\...\",\"\")\n",
    "   string =string.replace(\"'||\",\" '||\")\n",
    "   string =string.replace(\",\",\" , \")\n",
    "   string =string.replace(\"!\",\" ! \")\n",
    "   string =string.replace(\"(\",\" ( \")\n",
    "   string =string.replace(\")\",\" ) \")\n",
    "   string =string.replace(\"?\",\" ? \")\n",
    "   string =string.replace(\"\\s{2,}\",\"\")\n",
    "   string =string.replace(\" \",\"\")\n",
    "   string = string.lower()\n",
    "   return string\n",
    "    \n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "#### YOUR CODE HERE ####\n",
    "   re.sub(r\"\\'||\",\" \\'||\", string )\n",
    "   re.sub(r\",\",\" , \", string)\n",
    "   re.sub(r\"!\",\" ! \", string)\n",
    "   re.sub(r\"\\(\",r\" \\( \", string) \n",
    "   re.sub(r\"\\)\",\" ) \", string)\n",
    "   re.sub(r\"\\?\",r\" \\? \", string)\n",
    "   re.sub(r\"\\s{2,}\",\"\", string)\n",
    "   string =re.sub(\" \",\"\", string ) \n",
    "   string = string.lower()\n",
    "   return string\n",
    "    \n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfJb1RPSTihE"
   },
   "source": [
    "Kiểm tra kết quả với hàm vừa viết trên dữ liệu đã trích xuất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "JJbIFWXhTihE",
    "outputId": "e1d04039-b9f7-4c59-855d-3c5425ef15f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xửlýngônngữtựnhiên(nlp)liênquanđếnviệcxâydựngcácthuậttoántínhtoánđểtựđộngphântíchvàthểhiệnngônngữcủaconngười.cáchệthốngdựatrênnlpđãkíchhoạtmộtloạtcácứngdụngnhưcôngcụtìmkiếmmạnhmẽcủagoogle,vàgầnđây,trợlýgiọngnóicủaamazon,cótênalexa.nlpcũngtrởnênhữuíchđểdạychomáymóckhảnăngthựchiệncácnhiệmvụliênquanđếnngônngữtựnhiênphứctạpnhưdịchmáyvàtạohộithoại.trongmộtthờigiandài,phầnlớncácphươngphápđượcsửdụngđểnghiêncứucácvấnđềnlpsửdụngcácmôhìnhhọcmáynôngvàcáctínhnăngthủcông,tốnthờigian.điềunàydẫnđếncácvấnđềnhưlờinguyềnvềchiềuhướngvìthôngtinngônngữđượcthểhiệnbằngcácbiểuhiệnthưathớt(tínhnăngchiềucao).tuynhiên,vớisựphổbiếnvàthànhcônggầnđâycủacáctừnhúng(biểudiễnphântán,chiềuthấp),cácmôhìnhdựatrênthầnkinhđãđạtđượckếtquảvượttrộitrêncáctácvụliênquanđếnngônngữkhácnhausovớicácmôhìnhhọcmáytruyềnthốngnhưsvmhoặchồiquylogistic.từnhúng(wordembeddings):cácvectơphânphối,cònđượcgọilànhúngtừ,dựatrêncáigọilàgiảthuyếtphânphối-cáctừxuấthiệntrongngữcảnhtươngtựcóýnghĩatươngtự.cáctừnhúngđượcđàotạotrướcvềmộtnhiệmvụtrongđómụctiêulàdựđoánmộttừdựatrênngữcảnhcủanó,thườngsửdụngmộtmạnglướithầnkinhnông.hìnhdướiđâyminhhọamộtmôhìnhngônngữthầnkinhđượcđềxuấtbởibengiovàcácđồngnghiệpword2vec:khoảngnăm2013,mikolavvàcộngsự,đãđềxuấtcảmôhìnhcbowvàskip-gram.cbowlàmộtcáchtiếpcậnthầnkinhđểxâydựngcáctừnhúngvàmụctiêulàtínhxácsuấtcóđiềukiệncủamộttừmụctiêuchocáctừngữcảnhtrongmộtkíchthướccửasổnhấtđịnh.mặtkhác,skip-gramlàmộtcáchtiếpcậnthầnkinhđểxâydựngcáctừnhúng,trongđómụctiêulàdựđoáncáctừngữcảnhxungquanh(nghĩalàxácsuấtcóđiềukiện)đượcđưaramộttừmụctiêutrungtâm.đốivớicảhaimôhình,kíchthướcnhúngtừđượcxácđịnhbằngtínhtoán(theocáchkhônggiámsát)độchínhxáccủadựđoán.cnnvềcơbảnlàmộtcáchtiếpcậndựatrênthầnkinhđạidiệnchomộtchứcnăngtínhnăngđượcápdụngđểcấuthànhcáctừhoặcmôhìnhngônngữ(n-gram)đểtríchxuấtcáctínhnăngcấpcaohơn.cáctínhnăngtrừutượngkếtquảđãđượcsửdụnghiệuquảđểphântíchtìnhcảm,dịchmáyvàtrảlờicâuhỏi,trongsốcácnhiệmvụkhác.collobertvàwestonlàmộttrongnhữngnhànghiêncứuđầutiênápdụngcáckhungdựatrêncnnchocácnhiệmvụnlp.mụctiêucủaphươngphápcủahọlàbiếnđổicáctừthànhbiểudiễnvectơthôngquabảngtracứu,dẫnđếncáchtiếpcậnnhúngtừnguyênthủyđểhọccáctrọngsốtrongquátrìnhđàotạomạng(xemhìnhbêndưới).rnnlàcácphươngphápdựatrênnơ-ronchuyênbiệtcóhiệuquảtrongviệcxửlýthôngtintuầntự.mộtrnnápdụngđệquymộttínhtoánchomọitrườnghợpcủachuỗiđầuvàocóđiềukiệndựatrêncáckếtquảđượctínhtoántrướcđó.cácchuỗinàythườngđượcbiểuthịbằngmộtvectơkíchthướccốđịnhđượccungcấptuầntự(từngcáimột)chomộtđơnvịđịnhkỳ\n"
     ]
    }
   ],
   "source": [
    "print (clean_str(dataJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "les2_class_asm-sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1351fcf4d9a71a0a7642ff753eb3792ce6ab1ef3fbcd1886aadc4d216688484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
