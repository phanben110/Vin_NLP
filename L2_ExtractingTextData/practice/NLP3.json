{
  "news1": "Xử lý ngôn ngữ tự nhiên (NLP) liên quan đến việc xây dựng các thuật toán tính toán để tự động phân tích và thể hiện ngôn ngữ của con người. Các hệ thống dựa trên NLP đã kích hoạt một loạt các ứng dụng như công cụ tìm kiếm mạnh mẽ của Google, và gần đây, trợ lý giọng nói của Amazon, có tên Alexa. NLP cũng trở nên hữu ích để dạy cho máy móc khả năng thực hiện các nhiệm vụ liên quan đến ngôn ngữ tự nhiên phức tạp như dịch máy và tạo hội thoại.",
  "news2": "Trong một thời gian dài, phần lớn các phương pháp được sử dụng để nghiên cứu các vấn đề NLP sử dụng các mô hình học máy nông và các tính năng thủ công, tốn thời gian. Điều này dẫn đến các vấn đề như lời nguyền về chiều hướng vì thông tin ngôn ngữ được thể hiện bằng các biểu hiện thưa thớt (tính năng chiều cao). Tuy nhiên, với sự phổ biến và thành công gần đây của các từ nhúng (biểu diễn phân tán, chiều thấp), các mô hình dựa trên thần kinh đã đạt được kết quả vượt trội trên các tác vụ liên quan đến ngôn ngữ khác nhau so với các mô hình học máy truyền thống như SVM hoặc hồi quy logistic.",
  "news3": "Từ nhúng (Word Embeddings): Các vectơ phân phối, còn được gọi là nhúng từ, dựa trên cái gọi là giả thuyết phân phối - các từ xuất hiện trong ngữ cảnh tương tự có ý nghĩa tương tự. Các từ nhúng được đào tạo trước về một nhiệm vụ trong đó mục tiêu là dự đoán một từ dựa trên ngữ cảnh của nó, thường sử dụng một mạng lưới thần kinh nông. Hình dưới đây minh họa một mô hình ngôn ngữ thần kinh được đề xuất bởi Bengio và các đồng nghiệp",
  "news4": "Word2vec: Khoảng năm 2013, Mikolav và cộng sự, đã đề xuất cả mô hình CBOW và Skip-gram. CBOW là một cách tiếp cận thần kinh để xây dựng các từ nhúng và mục tiêu là tính xác suất có điều kiện của một từ mục tiêu cho các từ ngữ cảnh trong một kích thước cửa sổ nhất định. Mặt khác, Skip-gram là một cách tiếp cận thần kinh để xây dựng các từ nhúng, trong đó mục tiêu là dự đoán các từ ngữ cảnh xung quanh (nghĩa là xác suất có điều kiện) được đưa ra một từ mục tiêu trung tâm. Đối với cả hai mô hình, kích thước nhúng từ được xác định bằng tính toán (theo cách không giám sát) độ chính xác của dự đoán.",
  "news5": "CNN về cơ bản là một cách tiếp cận dựa trên thần kinh đại diện cho một chức năng tính năng được áp dụng để cấu thành các từ hoặc mô hình ngôn ngữ (n-gram) để trích xuất các tính năng cấp cao hơn. Các tính năng trừu tượng kết quả đã được sử dụng hiệu quả để phân tích tình cảm, dịch máy và trả lời câu hỏi, trong số các nhiệm vụ khác. Collobert và Weston là một trong những nhà nghiên cứu đầu tiên áp dụng các khung dựa trên CNN cho các nhiệm vụ NLP. Mục tiêu của phương pháp của họ là biến đổi các từ thành biểu diễn vectơ thông qua bảng tra cứu, dẫn đến cách tiếp cận nhúng từ nguyên thủy để học các trọng số trong quá trình đào tạo mạng (xem hình bên dưới).",
  "news6": "RNN là các phương pháp dựa trên nơ-ron chuyên biệt có hiệu quả trong việc xử lý thông tin tuần tự. Một RNN áp dụng đệ quy một tính toán cho mọi trường hợp của chuỗi đầu vào có điều kiện dựa trên các kết quả được tính toán trước đó. Các chuỗi này thường được biểu thị bằng một vectơ kích thước cố định được cung cấp tuần tự (từng cái một) cho một đơn vị định kỳ"
}