{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"les13_home_sol.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q6Iu9BoERWaD"},"source":["### **Bài 13: Bài tập về nhà: Ứng dụng mô hình sequence2sequence cho bài toán sinh văn bản (text generation)**\n","Tổng quan: Ở bài tập này chúng ta sẽ ôn lại cách xây dựng và sử dụng mô hình seq2seq cho bài toán sinh văn bản."]},{"cell_type":"markdown","metadata":{"id":"YiPDIh18dWVt"},"source":["**1. Chuẩn bị dữ liệu và tiền xử lý**\n","\n","Trong bài tập này chúng ta sẽ xử lý dữ liệu của bài toán tóm tắt văn bản để thực nghiệm cho bài toán sinh văn bản. Trong bài toán tóm tắt văn bản, input của chương trình sẽ là 1 văn bản dài và output sẽ là 1 văn bản ngắn hơn và chứa những thông tin quan trọng của văn bản đầu vào. Ngược lại với bài toán trên, trong bài toán sinh văn bản, chúng ta muốn input đầu vào là 1 vài keyword hoặc 1 đoạn văn ngắn và output ra 1 đoạn văn dài. Vì thế, ta hoàn toàn có thể sử dụng dữ liệu trong bài toán tóm tắt văn bản để huấn luyện cho bài toán sinh văn bản, với input là câu đã được tóm tắt và output là đoạn văn gốc."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghWGqPOLeQj9","outputId":"e84fb783-4ad3-4baa-82d1-0daad245ce5c","executionInfo":{"status":"ok","timestamp":1655912121129,"user_tz":-420,"elapsed":56525,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/\n","!mkdir bai13_home\n","%cd /content/drive/MyDrive/bai13_home"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7lqmpEVeXjN","outputId":"544d834e-f641-4a5b-9a1b-745d14865c62","executionInfo":{"status":"ok","timestamp":1655912144065,"user_tz":-420,"elapsed":311,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","/content/drive/MyDrive/bai13_home\n"]}]},{"cell_type":"code","metadata":{"id":"8vUbu5jijD8T"},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9mVYgn2fVy5"},"source":["Bài tập 1: Bạn hãy download 1 bộ dữ liệu tóm tắt văn bản và tiền xử lý chúng. Có thể dùng 1 trong các bộ dữ liệu dưới đây\n","*   Bộ gigaword : https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n","*   Bộ CNN/DM : https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n","\n","Yêu cầu : Sau khi tiền xử lý, chúng ta sẽ có 4 file data gồm :\n","*   train.input.txt : Chứa các câu tóm tắt dùng để huấn luyện mô hình, thường chiếm 80% kích thước tổng dữ liệu\n","*   train.output.txt : Chứa các đoạn văn bản gốc ứng với các tóm tắt.\n","*   valid.input.txt : Chứa các câu tóm tắt dùng để đánh giá mô hình, thường chiếm 10% kích thưởng tổng dữ liệu.\n","*   valid.output.txt : Chứa các đoạn văn bản gốc ứng vs các tóm tắt.\n","\n","Lưu ý : Nếu bạn để max_length của dữ liệu quá lớn, thì mô hình của bạn sẽ rất to và có thể gây tràn RAM.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0-A3MSS-H8F","outputId":"1f21980b-b731-4d66-8804-7c0fc402896b","executionInfo":{"status":"ok","timestamp":1655912187732,"user_tz":-420,"elapsed":34024,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"source":["!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n","!tar -xvzf cnndm.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-22 15:35:53--  https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.226.139\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.226.139|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 500375629 (477M) [application/x-gzip]\n","Saving to: ‘cnndm.tar.gz’\n","\n","cnndm.tar.gz        100%[===================>] 477.20M  51.7MB/s    in 8.5s    \n","\n","2022-06-22 15:36:02 (56.3 MB/s) - ‘cnndm.tar.gz’ saved [500375629/500375629]\n","\n","test.txt.src\n","test.txt.tgt.tagged\n","train.txt.src\n","train.txt.tgt.tagged\n","val.txt.src\n","val.txt.tgt.tagged\n"]}]},{"cell_type":"markdown","metadata":{"id":"yOQ0yGyEfbsw"},"source":["### Câu hỏi 1: Viết hàm tách từ và đọc dữ liệu "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ai4Rg9N8fL7q","outputId":"46901d78-dec1-4846-f0bf-a69d73d09511","executionInfo":{"status":"ok","timestamp":1655912204614,"user_tz":-420,"elapsed":10353,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"source":["train_src_file = \"train.txt.tgt.tagged\"\n","train_tgt_file = \"train.txt.src\"\n","valid_src_file = \"val.txt.tgt.tagged\"\n","valid_tgt_file = \"val.txt.src\"\n","\n","max_length_target = 50\n","max_length_source = 10\n","\n","# Thêm start và end token vào mỗi câu\n","def preprocess_sentence(sentence):\n","  sentence = \"<start> \"+sentence+\" <end>\"\n","  return sentence\n","\n","# Hàm load dữ liệu\n","def load_data(source_file, target_file, number_of_examples):\n","  f = open(source_file, \"r\")\n","  source_sents = f.readlines()\n","  f0 = open(target_file, \"r\")\n","  target_sents = f0.readlines()\n","  assert len(source_sents) == len(target_sents)\n","\n","  source_data, target_data = [], []\n","  for source_sentence, target_sentence in zip(source_sents[:number_of_examples],\n","                                              target_sents[:number_of_examples]):\n","    if len(source_sentence.strip().split()) > max_length_source:\n","      source_sentence = \" \".join(source_sentence.strip().split()[:max_length_source])\n","    if len(target_sentence.strip().split()) > max_length_target:\n","      target_sentence = \" \".join(target_sentence.strip().split()[:max_length_target])\n","    \n","    source_data.append(preprocess_sentence(source_sentence))\n","    target_data.append(preprocess_sentence(target_sentence))\n","  \n","  return source_data, target_data\n","\n","# Viết hàm tách từ với chuyển chữ in hoa thành in thường\n","# Hàm tách token\n","def tokenizer(sentences):\n","  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='', oov_token = \"unk\", lower = True)\n","  tokenizer.fit_on_texts(sentences)\n","\n","  sent_tensors = tokenizer.texts_to_sequences(sentences)\n","  sent_tensors = tf.keras.preprocessing.sequence.pad_sequences(sent_tensors,\n","                                                         padding='post')\n","  \n","  return sent_tensors, tokenizer\n","\n","# Tổng hợp các hàm trên thành một hàm đọc và xử lý dữ liệu\n","def create_data(source_path, target_path, number_of_examples):\n","  source_data, target_data = load_data(source_path, target_path, number_of_examples)\n","  source_tensors, source_tokenizer = tokenizer(source_data)\n","  target_tensors, target_tokenizer = tokenizer(target_data)\n","  return source_tensors, target_tensors, source_tokenizer, target_tokenizer\n","\n","number_of_examples = 10000\n","train_src_tensors, train_tgt_tensors, train_src_tokenizer, train_tgt_tokenizer = create_data(train_src_file, train_tgt_file, number_of_examples)\n","valid_src_tensors, valid_tgt_tensors, _, _ = create_data(valid_src_file, valid_tgt_file, -1)\n","\n","# max_length_source, max_length_target = train_src_tensors.shape[1], train_tgt_tensors.shape[1]\n","\n","print(len(train_src_tensors), len(valid_src_tensors))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000 13367\n"]}]},{"cell_type":"markdown","metadata":{"id":"jntYQNQDhQJy"},"source":["### Câu hỏi 2: Tạo batch dữ liệu\n","Sử dụng tf.data.Dataset để tạo dữ liệu huấn luyện**\n","Hãy xem lại bài tập seq2seq cho bài toán dịch máy để thực hiện cách build data theo batch."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sh86ntmfdQoF","outputId":"2c6defff-a7ec-45d8-b676-5852c1bc2b60","executionInfo":{"status":"ok","timestamp":1655912212805,"user_tz":-420,"elapsed":4197,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"source":["BUFFER_SIZE = len(train_src_tensors)\n","BATCH_SIZE = 64\n","\n","steps_per_epoch = len(train_src_tensors)//BATCH_SIZE\n","vocab_src_size = len(train_src_tokenizer.word_index)+1\n","vocab_tgt_size = len(train_tgt_tokenizer.word_index)+1\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_src_tensors, train_tgt_tensors)).shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# Print shape of the first batch\n","example_input_batch, example_target_batch = next(iter(train_dataset))\n","example_input_batch.shape, example_target_batch.shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 12]), TensorShape([64, 52]))"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"boAbxfdYh25B"},"source":["## Câu hỏi 3. Mô hình Seq2Seq với Attention**\n","\n","Bài tập 3: Hãy viết lại các thành phần Encoder, Attention, Decoder theo cách hiểu của bạn(Sử dụng LuongAttention).\n","## Câu hỏi 3.1 Xây dựng encoder"]},{"cell_type":"code","metadata":{"id":"x6EdqDh2iXg4"},"source":["#Viết hàm Encoder với mạng RNN\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.hidden_state_size = hidden_state_size\n","    # The embedding layer converts tokens to vectors\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    # The GRU RNN layer processes those vectors sequentially.\n","    self.simpleRNN = tf.keras.layers.SimpleRNN(self.hidden_state_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    # YOUR CODE HERE\n","    x = self.embedding(x)\n","    output, state = self.simpleRNN(x, initial_state = hidden)\n","    # Returns the new sequence and its state.\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.hidden_state_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymN9iH4bfbs0"},"source":["### Câu hỏi 3.2 Xây dựng lớp attention"]},{"cell_type":"code","metadata":{"id":"rD5mE4klfbs1"},"source":["class LuongAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(LuongAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    values_transposed = tf.transpose(values, perm=[0, 2, 1])\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hn84Ei2Rfbs1"},"source":["### Câu hỏi 3.3 Xây dựng lớp decoder"]},{"cell_type":"code","metadata":{"id":"H-JGpCcTfbs2"},"source":["#Viết hàm Decoder với mạng RNN\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.hidden_state_size = hidden_state_size\n","    # The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.simpleRNN = tf.keras.layers.SimpleRNN(self.hidden_state_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = LuongAttention(self.hidden_state_size)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the RNN\n","    output, state = self.simpleRNN(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rWDlRvCiuk5"},"source":["## Câu hỏi 3.4 Xây dựng hàm lỗi và tối ưu\n","\n","Hàm tối ưu SGD và hàm lỗi Cross Entropy được dùng rất phổ biến trong các mô hình học sâu, trong phần này, chúng ra sẽ dùng lại các hàm đó nhé."]},{"cell_type":"code","metadata":{"id":"PYs4tec9i_k6"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","  # Mask off the losses on padding.\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_) # Return the total\n","optimizer = tf.keras.optimizers.SGD()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UOgKpkcjJXo"},"source":["## Câu hỏi 4 Huấn luyện\n","\n","Giờ đã có đủ nguyên liệu và mô hình rồi, ta hãy cùng huấn luyện 1 mô hình có thể sinh văn bản."]},{"cell_type":"code","metadata":{"id":"SuZoHVLqjr70"},"source":["@tf.function\n","def train_step(source, target, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(source, enc_hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    for t in range(1, target.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(target[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(target[:, t], 1)\n","  \n","  batch_loss = (loss / int(target.shape[1]))\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gvVVAE_XeOs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655912296287,"user_tz":-420,"elapsed":65730,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"09fe03d2-d60e-4efa-e42a-ca6d2337ae52"},"source":["embedding_dim = 512\n","hidden_state_size = 512\n","\n","# YOUR CODE HERE\n","encoder = Encoder(vocab_src_size, embedding_dim, hidden_state_size, BATCH_SIZE)\n","decoder = Decoder(vocab_tgt_size, embedding_dim, hidden_state_size, BATCH_SIZE)\n","\n","checkpoint_dir = './model_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","EPOCHS = 1\n","print(\"start training ... \")\n","print(steps_per_epoch)\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","  for (batch, (source, target)) in enumerate(train_dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(source, target, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 1 epochs\n","  checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start training ... \n","156\n","Epoch 1 Batch 0 Loss 10.0539\n","Epoch 1 Batch 100 Loss 19.3011\n","Epoch 1 Loss 19.6054\n","Time taken for 1 epoch 65.18843841552734 sec\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"xSClgbvGjMOR"},"source":["## Câu hỏi 5 Sử dụng mô hình vừa được huấn luyện để sinh văn bản\n","\n","Hãy sử dụng mô hình vừa được huấn luyện để thực hiện sinh văn bản."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czLOA91zmlCm","outputId":"32e8c347-644c-46d7-9802-994a1adc7f3d","executionInfo":{"status":"ok","timestamp":1655912311120,"user_tz":-420,"elapsed":735,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"source":["def predict(input_sentence):\n","  attention_plot = np.zeros((max_length_target, max_length_source))\n","\n","  input_sentence = preprocess_sentence(input_sentence)\n","  inputs = []\n","  for i in input_sentence.lower().split(' '):\n","    if i not in train_src_tokenizer.word_index:\n","      inputs.append(train_src_tokenizer.word_index['unk'])\n","    else:\n","      inputs.append(train_src_tokenizer.word_index[i])\n","\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_source,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  hidden = [tf.zeros((1, hidden_state_size))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']], 0)\n","\n","  for t in range(max_length_target):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += train_tgt_tokenizer.index_word[predicted_id] + ' '\n","    # Stop predicting when the model predicts the end token.\n","    if train_tgt_tokenizer.index_word[predicted_id] == '<end>':\n","      return result, source_sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","  return result, input_sentence, attention_plot\n","\n","input_sentence = \"hollywood shores up support for ocean 's thirteen\"\n","original_article = \"hollywood is planning a new sequel to adventure flick `` ocean 's eleven , '' with star george clooney set to reprise his role as a charismatic thief in `` ocean 's thirteen , '' the entertainment press said wednesday .\"\n","result, input_sentence, attention_plot = predict(input_sentence)\n","print('Input: %s' % (input_sentence))\n","print('Output : {}'.format(result))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: <start> hollywood shores up support for ocean 's thirteen <end>\n","Output : -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- -lrb- \n"]}]}]}