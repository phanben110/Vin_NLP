{"cells":[{"cell_type":"markdown","metadata":{"id":"Zv_0feSa73x6"},"source":["## **Bài thực hành bài 12**: Ứng dụng mô hình sequence2sequence cho bài toán dịch máy (machine translation)\n","\n","Tổng quan: Ở bài tập này chúng ta sẽ lần lượt thực hành các bước để xây dựng một hệ thống học máy cho bài toán dịch máy dựa vào mạng seq2seq: tải và tiền xử lý dữ liệu song ngữ, tạo dữ liệu huấn luyện, xây dựng mô hình seq2seq với attention, hiển thị dữ liệu attention và dịch câu mới trên dữ liệu thực tế. Bài tập yêu cầu các kiến thức về lập trình Python với các thư viện: keras, numpy."]},{"cell_type":"markdown","metadata":{"id":"QplquiDdApB0"},"source":["**1. Tải và tiền xử lý dữ liệu**\n","\n","Nội dung chính: \n","1.   Tải dữ liệu song ngữ\n","2.   Xóa các kí tự đặc biệt trong câu\n","3.   Tách từ (word tokenizer)\n","4.   Tạo từ điển\n","5.   Thêm các token đặc biệt vào câu như : start, end, pad\n","\n","\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycD3Hsoz5yF9","outputId":"2acdc8b1-3f1f-4e05-c27f-102845df0d73","executionInfo":{"status":"ok","timestamp":1655910591890,"user_tz":-420,"elapsed":21718,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/\n","!mkdir bai13 \n","%cd /content/drive/MyDrive/bai13"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlByxcNS7KyQ","outputId":"5677f9cc-b9da-4b2e-c648-4b284c3d3817","executionInfo":{"status":"ok","timestamp":1655910598955,"user_tz":-420,"elapsed":383,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","/content/drive/MyDrive/bai13\n"]}]},{"cell_type":"markdown","metadata":{"id":"u5OxzO0WtOXp"},"source":["- Import các thư viện cần thiết"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nRPUxQvLtSwx","executionInfo":{"status":"ok","timestamp":1655910797553,"user_tz":-420,"elapsed":3095,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"]},{"cell_type":"markdown","metadata":{"id":"zoEhyJ6owUcV"},"source":["- Dowload dataset\n","\n","Trong bài thực hành này chúng ra sẽ sử dụng bộ song ngữ Vietnamese - English gồm 133k cặp câu song ngữ"]},{"cell_type":"code","source":["OUT_DIR = \"data_iwslt15\"\n","!mkdir $OUT_DIR\n","SITE_PREFIX=\"https://nlp.stanford.edu/projects/nmt/data\"\n","# Download iwslt15 small dataset from standford website.\n","print (\"Download training dataset train.en and train.vi.\")\n","!curl -o \"$OUT_DIR/train.en\" \"$SITE_PREFIX/iwslt15.en-vi/train.en\"\n","!curl -o \"$OUT_DIR/train.vi\" \"$SITE_PREFIX/iwslt15.en-vi/train.vi\"\n","\n","print (\"Download dev dataset tst2012.en and tst2012.vi.\")\n","!curl -o \"$OUT_DIR/tst2012.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.en\"\n","!curl -o \"$OUT_DIR/tst2012.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.vi\"\n","\n","print (\"Download test dataset tst2013.en and tst2013.vi.\")\n","!curl -o \"$OUT_DIR/tst2013.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.en\"\n","!curl -o \"$OUT_DIR/tst2013.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.vi\""],"metadata":{"id":"1nRs4OqASxSC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910820975,"user_tz":-420,"elapsed":18330,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"0f21f50c-aa4d-458b-96d2-4b4df468de05"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Download training dataset train.en and train.vi.\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 12.9M  100 12.9M    0     0  2241k      0  0:00:05  0:00:05 --:--:-- 3259k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 17.2M  100 17.2M    0     0  3124k      0  0:00:05  0:00:05 --:--:-- 3826k\n","Download dev dataset tst2012.en and tst2012.vi.\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  136k  100  136k    0     0   101k      0  0:00:01  0:00:01 --:--:--  101k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  183k  100  183k    0     0   122k      0  0:00:01  0:00:01 --:--:--  122k\n","Download test dataset tst2013.en and tst2013.vi.\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  129k  100  129k    0     0  97900      0  0:00:01  0:00:01 --:--:-- 97828\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  179k  100  179k    0     0   132k      0  0:00:01  0:00:01 --:--:--  132k\n"]}]},{"cell_type":"markdown","metadata":{"id":"Da97Qlc91-9i"},"source":["- Viết hàm tiền xử lý dữ liệu. Hàm này xử lý dữ liệu đầu vào như xóa bỏ các kí tự đặc biệt, lowercase, ... Ngoài ra ta cũng thêm token bắt đầu và kết thúc câu."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"s-YZuigf2OLg","executionInfo":{"status":"ok","timestamp":1655910829576,"user_tz":-420,"elapsed":391,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["def preprocess_sentence(sentence):\n","  sentence = \"<start> \"+sentence+\" <end>\"\n","  return sentence\n"]},{"cell_type":"markdown","source":["- Load data và xem thử data"],"metadata":{"id":"JrGZx0hQSDed"}},{"cell_type":"code","source":["def load_data(source_file, target_file, number_of_examples):\n","  # source_data = [preprocess_sentence(source_sentence) \n","  #                 for source_sentence in open(source_file, \"r\").readlines()[:number_of_examples]]\n","  # target_data = [preprocess_sentence(target_sentence) \n","  #                 for target_sentence in open(target_file, \"r\").readlines()[:number_of_examples]]\n","  max_len = 50\n","  source_sents = open(source_file, \"r\").readlines()\n","  target_sents = open(target_file, \"r\").readlines()\n","  assert len(source_sents) == len(target_sents)\n","\n","  source_data, target_data = [], []\n","  for source_sentence, target_sentence in zip(source_sents[:number_of_examples],\n","                                              target_sents[:number_of_examples]):\n","    if len(source_sentence.strip().split()) > max_len or len(target_sentence.strip().split()) > max_len:\n","      continue\n","    source_data.append(preprocess_sentence(source_sentence))\n","    target_data.append(preprocess_sentence(target_sentence))\n","  \n","  return source_data, target_data\n","\n","train_src, train_trg = load_data(OUT_DIR+\"/train.en\", OUT_DIR+\"/train.vi\", 1000)\n","print(len(train_src), len(train_trg))\n","print(train_src[10])\n","print(train_trg[10])\n"],"metadata":{"id":"tkeP7AwlR-GV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910830800,"user_tz":-420,"elapsed":3,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"0d71d843-72f2-422c-d2c9-143c32798676"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["929 929\n","<start> And every one of those scientists is in a research group , and every research group studies a wide variety of topics .\n"," <end>\n","<start> Mỗi một khoa học gia đều thuộc một nhóm nghiên cứu , và mỗi nhóm đều nghiên cứu rất nhiều đề tài đa dạng .\n"," <end>\n"]}]},{"cell_type":"markdown","metadata":{"id":"F0OhXShZ1X0B"},"source":["## Câu hỏi 1:  Viết hàm tách từ (tokenizer).\n","Nhiều ngôn ngữ có thể phân tách các từ bằng dấu cách (space) như tiếng Anh, tiếng Việt, ... Nhưng 1 số ngôn ngữ không có dấu phân tách giữa các từ như tiếng Nhật, tiếng Trung Quốc. Vì vậy, ta cần có tách từ để phân tách các từ trong câu."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-sD8Z74u11uJ","executionInfo":{"status":"ok","timestamp":1655910834198,"user_tz":-420,"elapsed":362,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["#Viết hàm tách từ với việc xử lý chuyển chữ in hoa thành chữ thường, với những từ không xuất hiện trong vocab sẽ được thay bằng UNK\n","def tokenizer(sentences):\n","  # YOUR CODE HERE\n","  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='',oov_token = \"UNK\",lower = True)\n","  tokenizer.fit_on_texts(sentences)\n","\n","  sent_tensors = tokenizer.texts_to_sequences(sentences)\n","\n","  # write code to pad sequence here\n","  sent_tensors = tf.keras.preprocessing.sequence.pad_sequences(sent_tensors,\n","                                                         padding='post')\n","  # YOUR CODE HERE\n","  \n","  return sent_tensors, tokenizer\n","\n","def create_data(source_path, target_path, number_of_examples):\n","  # creating cleaned input, output pairs\n","  source_data, target_data = load_data(source_path, target_path, number_of_examples)\n","  source_tensors, source_tokenizer = tokenizer(source_data)\n","  target_tensors, target_tokenizer = tokenizer(target_data)\n","\n","  return source_tensors, target_tensors, source_tokenizer, target_tokenizer"]},{"cell_type":"markdown","metadata":{"id":"UGNuizCe_mMk"},"source":["- Giới hạn số dữ liệu huấn luyện. Để kiểm tra xem chương trình chạy có đúng hay không, ban đầu ta có thể chạy thử với lượng nhỏ dữ liệu"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Wk_TG19Y_23y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910839887,"user_tz":-420,"elapsed":3188,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"e2ede779-90a6-480e-ad4c-70290b2abc0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["27630\n","20208\n","2 <start>\n","8145 rachel\n","11265 pike\n","62 :\n","6 the\n"]}],"source":["number_of_examples = 30000\n","train_src_tensors, train_tgt_tensors, train_src_tokenizer, train_tgt_tokenizer = create_data(OUT_DIR+\"/train.en\", OUT_DIR+\"/train.vi\", number_of_examples)\n","valid_src_tensors, valid_tgt_tensors, _, _ = create_data(OUT_DIR+\"/tst2012.en\", OUT_DIR+\"/tst2012.vi\", -1)\n","test_src_tensors, test_tgt_tensors, _, _ = create_data(OUT_DIR+\"/tst2013.en\", OUT_DIR+\"/tst2013.vi\", -1)\n","\n","max_length_source, max_length_target = train_src_tensors.shape[1], train_tgt_tensors.shape[1]\n","\n","print(len(train_src_tensors))\n","# print(train_src_tensors[0])\n","\n","# View source dictionary\n","print(len(train_src_tokenizer.word_index))\n","for t in train_src_tensors[0][:5]:\n","  print(t, train_src_tokenizer.index_word[t])\n"]},{"cell_type":"markdown","metadata":{"id":"RB_awkp8A4Zh"},"source":["### Câu hỏi 2: Tạo dữ liệu huấn luyện**\n","\n","Nội dung chính: Tạo dữ liệu huấn luyện theo batch"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"24QafWymFXIb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910845614,"user_tz":-420,"elapsed":3787,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"01552668-aa52-4443-a459-0f6cffc4a8ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([32, 52]), TensorShape([32, 52]))"]},"metadata":{},"execution_count":9}],"source":["BUFFER_SIZE = len(train_src_tensors)\n","BATCH_SIZE = 32\n","steps_per_epoch = len(train_src_tensors)//BATCH_SIZE\n","\n","vocab_src_size = len(train_src_tokenizer.word_index)+1\n","vocab_tgt_size = len(train_tgt_tokenizer.word_index)+1\n","\n","# YOUR CODE HERE\n","def dataset():\n","  train = tf.data.Dataset.from_tensor_slices((train_src_tensors, train_tgt_tensors)).shuffle(BUFFER_SIZE)\n","  train = train.batch(BATCH_SIZE, drop_remainder=True)\n","  return train\n","# YOUR CODE HERE\n","\n","\n","example_input_batch, example_target_batch = next(iter(dataset()))\n","example_input_batch.shape, example_target_batch.shape"]},{"cell_type":"markdown","metadata":{"id":"2OhJaiJfBDj4"},"source":["### Câu hỏi 3. Viết mô hình seq2seq với attention**\n","Nội dung chính : Viết Encoder, BahdanauAttention, Decoder\n","\n","### Câu hỏi 3.1 Viết encoder"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Sx4JHLpcGz9E","executionInfo":{"status":"ok","timestamp":1655910851654,"user_tz":-420,"elapsed":352,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["# Viết hàm Encoder sử dụng mạng RNN\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    super(Encoder, self).__init__()\n","    # YOUR CODE HERE\n","    self.batch_sz = batch_sz\n","    self.hidden_state_size = hidden_state_size\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.simpleRNN = tf.keras.layers.SimpleRNN(self.hidden_state_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # YOUR CODE HERE\n","    \n","  def call(self, x, hidden):\n","    # YOUR CODE HERE\n","    x = self.embedding(x)\n","    output, state = self.simpleRNN(x, initial_state = hidden)\n","    # YOUR CODE HERE\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.hidden_state_size))"]},{"cell_type":"markdown","source":["- Tạo Encoder và test thử  Encoder đó với 1 batch dữ liệu và in ra shape của encoder output"],"metadata":{"id":"S2G_L0AZUoKE"}},{"cell_type":"code","source":["embedding_dim = 512\n","hidden_state_size = 512\n","\n","\n","encoder = Encoder(vocab_src_size, embedding_dim, hidden_state_size, BATCH_SIZE)\n","\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n","#"],"metadata":{"id":"awTPZmraUqG0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910862402,"user_tz":-420,"elapsed":2163,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"be092c2f-2abe-45b5-be18-dbca1ca0b3be"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder output shape: (batch size, sequence length, units) (32, 52, 512)\n","Encoder Hidden state shape: (batch size, units) (32, 512)\n"]}]},{"cell_type":"code","source":["class LuongAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(LuongAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    values_transposed = tf.transpose(values, perm=[0, 2, 1])\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"metadata":{"id":"hStc5wg2VL8i","executionInfo":{"status":"ok","timestamp":1655910866422,"user_tz":-420,"elapsed":4,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["attention_layer = LuongAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWs8fkowVNm9","outputId":"150725b9-2e60-46ae-89b2-89fb236d010c","executionInfo":{"status":"ok","timestamp":1655910870921,"user_tz":-420,"elapsed":365,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention result shape: (batch size, units) (32, 512)\n","Attention weights shape: (batch_size, sequence_length, 1) (32, 52, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"G0d4hd1hEFNh"},"source":["### Câu hỏi 3.2 Viết decoder"]},{"cell_type":"code","source":["#Viết hàm Decoder với mạng RNN\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    super(Decoder, self).__init__()\n","    # YOUR CODE HERE\n","    self.batch_sz = batch_sz\n","    self.hidden_state_size = hidden_state_size\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.simpleRNN = tf.keras.layers.SimpleRNN(self.hidden_state_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","    # YOUR CODE HERE\n","    \n","    # used for attention\n","    self.attention = LuongAttention(self.hidden_state_size)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the RNN\n","    output, state = self.simpleRNN(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"metadata":{"id":"_nhV25BQd0Sl","executionInfo":{"status":"ok","timestamp":1655910873799,"user_tz":-420,"elapsed":2,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"iERUOAAnIsDR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae70226a-58d7-49ec-c243-7721126cd23f","executionInfo":{"status":"ok","timestamp":1655910876965,"user_tz":-420,"elapsed":377,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder output shape: (batch_size, vocab size) (32, 9645)\n"]}],"source":["decoder = Decoder(vocab_tgt_size, embedding_dim, hidden_state_size, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"]},{"cell_type":"markdown","metadata":{"id":"knp_Kg5fBW9V"},"source":["### Câu hỏi 4: Định nghĩa hàm tối ưu (optimizer) và hàm lỗi (loss function)\n","\n","Nội dung chính: Sử dụng hàm tối ưu SGD và hàm lỗi CrossEntropy"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"CbmLqm3c8tgk","executionInfo":{"status":"ok","timestamp":1655910879697,"user_tz":-420,"elapsed":2,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  # YOUR CODE HERE\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  # YOUR CODE HERE\n","  \n","  return tf.reduce_mean(loss_)\n","optimizer = tf.keras.optimizers.SGD()"]},{"cell_type":"markdown","metadata":{"id":"GIAuA4oEBkIR"},"source":["### Câu hỏi 5. Huấn luyện mô hình (Training)\n","\n","Nội dung chính: Sử dụng dữ liệu, mô hình seq2seq + attention đã build ở trên để huấn luyện mô hình."]},{"cell_type":"code","source":["@tf.function\n","def train_step(source, target, enc_hidden):\n","  loss = 0\n","  # YOUR CODE HERE\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(source, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, target.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(target[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(target[:, t], 1)\n","  # YOUR CODE HERE\n","\n","\n","  batch_loss = (loss / int(target.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss\n","\n","\n","checkpoint_dir = './model_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","\n","EPOCHS = 1\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (source, target)) in enumerate(dataset().take(steps_per_epoch)):\n","    batch_loss = train_step(source, target, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 1 epochs\n","  checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6N61AG5KgukX","outputId":"48ebe36a-c8f2-46dc-d36d-a732d8de3069","executionInfo":{"status":"ok","timestamp":1655911001914,"user_tz":-420,"elapsed":120245,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 3.7656\n","Epoch 1 Batch 100 Loss 2.4638\n","Epoch 1 Batch 200 Loss 2.5951\n","Epoch 1 Batch 300 Loss 2.5634\n","Epoch 1 Batch 400 Loss 2.5783\n","Epoch 1 Batch 500 Loss 2.3103\n","Epoch 1 Batch 600 Loss 2.1152\n","Epoch 1 Batch 700 Loss 2.5691\n","Epoch 1 Batch 800 Loss 2.9593\n","Epoch 1 Loss 2.6612\n","Time taken for 1 epoch 120.10481905937195 sec\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y0dl31L1CAZT"},"source":["### Câu hỏi 6. Dịch (Translate)**\n","\n","Nội dung chính: Sử dụng mô hình đã được huấn luyện để  dịch dữ liệu mới"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"4eLxiM8JHdjh","executionInfo":{"status":"ok","timestamp":1655911005257,"user_tz":-420,"elapsed":365,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[],"source":["def predict(source_sentence):\n","  attention_plot = np.zeros((max_length_target, max_length_source))\n","\n","  source_sentence = preprocess_sentence(source_sentence)\n","\n","  inputs = [train_src_tokenizer.word_index[i] for i in source_sentence.lower().split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_source,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  # YOUR CODE HERE\n","  hidden = [tf.zeros((1, hidden_state_size))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([train_tgt_tokenizer.word_index['<start>']], 0)\n","\n","  for t in range(max_length_target):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += train_tgt_tokenizer.index_word[predicted_id] + ' '\n","  # YOUR CODE HERE\n","\n","\n","    if train_tgt_tokenizer.index_word[predicted_id] == '<end>':\n","      return result, source_sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, source_sentence, attention_plot\n","\n"]},{"cell_type":"code","source":["source_sentence = \"How are you ?\"\n","result, sent, attention_plot = predict(source_sentence)\n","print('Input: %s' % (sent))\n","print('Predicted translation: {}'.format(result))\n","\n"],"metadata":{"id":"9AlKqfBemEgZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655911007426,"user_tz":-420,"elapsed":3,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"9b39e752-c340-45ce-8388-88f99a60ab70"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Input: <start> How are you ? <end>\n","Predicted translation: và .\n"," <end> \n"]}]},{"cell_type":"markdown","metadata":{"id":"YlIHB7pICSJV"},"source":["**7. Hiển thị attention (Plot attention)**\n","\n","Nội dung chính : Hiển thị attention để hiểu rõ hơn vai trò của attention trong mô hình seq2seq\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":615},"id":"K_GAnSPZMZyl","outputId":"823d35fd-6a2a-42ab-efff-f20ca674f4d2","executionInfo":{"status":"ok","timestamp":1655911012508,"user_tz":-420,"elapsed":773,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x720 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["def plot_attention(attention, sentence, predicted_sentence):\n","  fig = plt.figure(figsize=(10,10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()\n","\n","attention_plot = attention_plot[:len(result.split(' ')), :len(source_sentence.split(' '))]\n","plot_attention(attention_plot, source_sentence.split(' '), result.split(' '))"]},{"cell_type":"markdown","metadata":{"id":"MZnHpT-LOI0h"},"source":["**8. Tính điểm BLEU**\n","\n","BLEU là 1 độ đo rất phổ biến để đánh giá chất lượng của 1 hệ thống dịch máy. Bạn có thể download script tính multi-bleu tại github của mosesdecoder : https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\n","\n","Trong bài này, chúng tôi sẽ sử dụng hàm tính bleu có sẵn trong python."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mka8zUX8PBLE","outputId":"f89cffd5-3997-4c76-fc2b-9ed2432677fc","executionInfo":{"status":"ok","timestamp":1655911017007,"user_tz":-420,"elapsed":724,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}],"source":["from nltk.translate.bleu_score import sentence_bleu\n","reference = [['this', 'is', 'a', 'test']]\n","candidate = ['this', 'is', 'a', 'test']\n","score = sentence_bleu(reference, candidate)\n","print(score)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"les13_class_sol.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}