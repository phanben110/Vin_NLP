{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"les13_home.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q6Iu9BoERWaD"},"source":["### **Bài 13: Bài tập về nhà: Ứng dụng mô hình sequence2sequence cho bài toán sinh văn bản (text generation)**\n","Tổng quan: Ở bài tập này chúng ta sẽ ôn lại cách xây dựng và sử dụng mô hình seq2seq cho bài toán sinh văn bản."]},{"cell_type":"markdown","metadata":{"id":"YiPDIh18dWVt"},"source":["**1. Chuẩn bị dữ liệu và tiền xử lý**\n","\n","Trong bài tập này chúng ta sẽ xử lý dữ liệu của bài toán tóm tắt văn bản để thực nghiệm cho bài toán sinh văn bản. Trong bài toán tóm tắt văn bản, input của chương trình sẽ là 1 văn bản dài và output sẽ là 1 văn bản ngắn hơn và chứa những thông tin quan trọng của văn bản đầu vào. Ngược lại với bài toán trên, trong bài toán sinh văn bản, chúng ta muốn input đầu vào là 1 vài keyword hoặc 1 đoạn văn ngắn và output ra 1 đoạn văn dài. Vì thế, ta hoàn toàn có thể sử dụng dữ liệu trong bài toán tóm tắt văn bản để huấn luyện cho bài toán sinh văn bản, với input là câu đã được tóm tắt và output là đoạn văn gốc."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"jZGkFYz4ESHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/\n","!mkdir bai13_home\n","%cd /content/drive/MyDrive/bai13_home"],"metadata":{"id":"EQE1fmrnETdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vUbu5jijD8T"},"source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9mVYgn2fVy5"},"source":["Bài tập 1: Bạn hãy download 1 bộ dữ liệu tóm tắt văn bản và tiền xử lý chúng. Có thể dùng 1 trong các bộ dữ liệu dưới đây\n","*   Bộ gigaword : https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n","*   Bộ CNN/DM : https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n","\n","Yêu cầu : Sau khi tiền xử lý, chúng ta sẽ có 4 file data gồm :\n","*   train.input.txt : Chứa các câu tóm tắt dùng để huấn luyện mô hình, thường chiếm 80% kích thước tổng dữ liệu\n","*   train.output.txt : Chứa các đoạn văn bản gốc ứng với các tóm tắt.\n","*   valid.input.txt : Chứa các câu tóm tắt dùng để đánh giá mô hình, thường chiếm 10% kích thưởng tổng dữ liệu.\n","*   valid.output.txt : Chứa các đoạn văn bản gốc ứng vs các tóm tắt.\n","\n","Lưu ý : Nếu bạn để max_length của dữ liệu quá lớn, thì mô hình của bạn sẽ rất to và có thể gây tràn RAM.\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n","!tar -xvzf cnndm.tar.gz"],"metadata":{"id":"bvrTErN5Ecj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Câu hỏi 1: Viết hàm tách từ và đọc dữ liệu "],"metadata":{"id":"nCWs8TSzEhSH"}},{"cell_type":"code","metadata":{"id":"ai4Rg9N8fL7q"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jntYQNQDhQJy"},"source":["### Câu hỏi 2: Tạo batch dữ liệu\n","Sử dụng tf.data.Dataset để tạo dữ liệu huấn luyện**\n","Hãy xem lại bài tập seq2seq cho bài toán dịch máy để thực hiện cách build data theo batch."]},{"cell_type":"code","metadata":{"id":"sh86ntmfdQoF"},"source":["BUFFER_SIZE = len(train_src_tensors)\n","BATCH_SIZE = 64\n","\n","steps_per_epoch = len(train_src_tensors)//BATCH_SIZE\n","# YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boAbxfdYh25B"},"source":["## Câu hỏi 3: Mô hình Seq2Seq với Attention**\n","\n","Bài tập 3: Hãy viết lại các thành phần Encoder, Attention, Decoder theo cách hiểu của bạn.(Sử dụng LuongAttention)\n","### Câu hỏi 3.1: Xây dựng encoder"]},{"cell_type":"code","metadata":{"id":"x6EdqDh2iXg4"},"source":["#Viết hàm Encoder với mạng RNN\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    # YOUR CODE HERE\n","\n","  def call(self, x, hidden):\n","    # YOUR CODE HERE\n","\n","  def initialize_hidden_state(self):\n","    # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obnfwyToe6fH"},"source":["### Câu hỏi 3.2: Xây dựng lớp attention"]},{"cell_type":"code","metadata":{"id":"9zm2wrSie6fI"},"source":["class LuongAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    # YOUR CODE HERE\n","\n","  def call(self, query, values):\n","    # YOUR CODE HERE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRcLhTL_e6fJ"},"source":["## Câu hỏi 3.3: Xây dựng lớp decoder"]},{"cell_type":"code","metadata":{"id":"3sLrkhwle6fJ"},"source":["#Viết hàm Decoder với mạng RNN\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, hidden_state_size, batch_sz):\n","    # YOUR CODE HERE\n","\n","  def call(self, x, hidden, enc_output):\n","    # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rWDlRvCiuk5"},"source":["## Câu hỏi 3.4: Xây dựng hàm lỗi và tối ưu\n","\n","Hàm tối ưu SGD và hàm lỗi Cross Entropy được dùng rất phổ biến trong các mô hình học sâu, trong phần này, chúng ra sẽ dùng lại các hàm đó nhé."]},{"cell_type":"code","metadata":{"id":"PYs4tec9i_k6"},"source":["# Khai báo optimizer\n","# Định nghĩa hàm loss\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UOgKpkcjJXo"},"source":["## Câu hỏi 4: Huấn luyện\n","\n","Giờ đã có đủ nguyên liệu và mô hình rồi, ta hãy cùng huấn luyện 1 mô hình có thể sinh văn bản."]},{"cell_type":"code","source":["@tf.function\n","def train_step(source, target, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = None\n","    dec_hidden = None\n","    dec_input = None\n","\n","    for t in range(1, target.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = None\n","\n","      loss += None\n","\n","      # using teacher forcing\n","      dec_input = None\n","  \n","  batch_loss = (loss / int(target.shape[1]))\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss\n"],"metadata":{"id":"fytQqzCtFClO"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuZoHVLqjr70"},"source":["embedding_dim = 512\n","hidden_state_size = 512\n","\n","# YOUR CODE HERE\n","\n","# YOUR CODE END HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSClgbvGjMOR"},"source":["## Câu hỏi 5: Sử dụng mô hình vừa được huấn luyện để sinh văn bản\n","\n","Hãy sử dụng mô hình vừa được huấn luyện để thực hiện sinh văn bản."]},{"cell_type":"code","metadata":{"id":"czLOA91zmlCm"},"source":["def predict(input_sentence):\n","  attention_plot = np.zeros((max_length_target, max_length_source))\n","  # YOUR CODE HERE\n","  input_sentence = none\n","  inputs = []\n","  for i in input_sentence.lower().split(' '):\n","    if i not in train_src_tokenizer.word_index:\n","      inputs.append(train_src_tokenizer.word_index['unk'])\n","    else:\n","      inputs.append(train_src_tokenizer.word_index[i])\n","\n","  inputs = None\n","  inputs = None\n","\n","  result = ''\n","\n","  hidden = None\n","  enc_out, enc_hidden = none\n","\n","  dec_hidden = None\n","  dec_input = None\n","\n","  for t in range(max_length_target):\n","    predictions, dec_hidden, attention_weights = None\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = None\n","    attention_plot[t] = None\n","\n","    predicted_id = None\n","\n","    result += None\n","    # Stop predicting when the model predicts the end token.\n","    if train_tgt_tokenizer.index_word[predicted_id] == '<end>':\n","      return result, source_sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","  return result, input_sentence, attention_plot\n","# YOUR CODE END HERE\n","input_sentence = \"hollywood shores up support for ocean 's thirteen\"\n","original_article = \"hollywood is planning a new sequel to adventure flick `` ocean 's eleven , '' with star george clooney set to reprise his role as a charismatic thief in `` ocean 's thirteen , '' the entertainment press said wednesday .\"\n","result, input_sentence, attention_plot = predict(input_sentence)\n","print('Input: %s' % (input_sentence))\n","print('Output : {}'.format(result))"],"execution_count":null,"outputs":[]}]}