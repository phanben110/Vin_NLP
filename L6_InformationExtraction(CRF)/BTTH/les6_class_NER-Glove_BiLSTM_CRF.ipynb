{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"les6_class_NER-Glove_BiLSTM_CRF.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Nhận dạng thực thể sử dụng thư viện Flair để xây dựng mô hình học sâu BiLSTM_CRF#\n","Ở bài trước ta đã thiết kế các đặc trưng và sử dụng mô hình CRF để huấn luyện. Ở bài này ta sẽ tiến hành sử xây dựng một mô hình học sâu theo kiến trúc Glove+BiLSTM+CRF để thực hiện bài toán NER"],"metadata":{"id":"5ref0mG7asDn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyWRU41Wn9X9","executionInfo":{"status":"ok","timestamp":1656015432785,"user_tz":-420,"elapsed":19762,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"b7deb956-9073-4a18-dfd3-bb26ff7f7d5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#Cài thư viện\n","!pip install flair==0.11.1\n","!pip install sacremoses"],"metadata":{"id":"dWqc9ovle_cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656015516525,"user_tz":-420,"elapsed":49151,"user":{"displayName":"NLP Team 1 AIacademy","userId":"13912621874855129227"}},"outputId":"912a3bca-e8a4-47f0-d5d4-67fc332221e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flair==0.11.1\n","  Downloading flair-0.11.1-py3-none-any.whl (401 kB)\n","\u001b[K     |████████████████████████████████| 401 kB 9.8 MB/s \n","\u001b[?25hCollecting more-itertools~=8.8.0\n","  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n","\u001b[K     |████████████████████████████████| 48 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (2.8.2)\n","Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (3.6.0)\n","Collecting pptree\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (0.8.9)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (4.2.6)\n","Collecting segtok>=1.5.7\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting hyperopt>=0.2.7\n","  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 56.4 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.95\n","  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 58.1 MB/s \n","\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n","  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n","Collecting deprecated>=1.2.4\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting janome\n","  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n","\u001b[K     |████████████████████████████████| 19.7 MB 409 kB/s \n","\u001b[?25hCollecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 60.4 MB/s \n","\u001b[?25hCollecting mpld3==0.3\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[K     |████████████████████████████████| 788 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (1.0.2)\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n","\u001b[?25hCollecting wikipedia-api\n","  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n","Collecting conllu>=4.0\n","  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n","Collecting transformers>=4.0.0\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 52.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (4.64.0)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (3.2.2)\n","Collecting sqlitedict>=1.6.0\n","  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 4.2 MB/s \n","\u001b[?25hCollecting bpemb>=0.3.2\n","  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (2022.6.2)\n","Collecting gdown==3.12.2\n","  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.11.1) (1.11.0+cu113)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.11.1) (3.7.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.11.1) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.11.1) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair==0.11.1) (1.21.6)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.11.1) (1.14.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.11.1) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.11.1) (5.2.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair==0.11.1) (0.16.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair==0.11.1) (1.3.0)\n","Collecting py4j\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 76.5 MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair==0.11.1) (2.6.3)\n","Collecting requests\n","  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n","\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n","  Downloading overrides-3.1.0.tar.gz (11 kB)\n","Collecting importlib-metadata<4.0.0,>=3.7.0\n","  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.11.1) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.11.1) (4.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.11.1) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.11.1) (1.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.11.1) (3.0.9)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.11.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.11.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.11.1) (2022.6.15)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.11.1) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.11.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.11.1) (1.1.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.11.1) (21.3)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.11.1) (0.2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.11.1) (1.7.1)\n","Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=cc8f0203bbc1ecd71603a878d9f960e524a2ae01e5380687ff972551d9faee93\n","  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=30b00fa5b6133e5e81d10eb5117b71155069fd270d14d6f90f7171fc1b152d0c\n","  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=701d86e50d7a37c0b31d4cd173f3bb0cb4ef11e14f513046ea1622ceabe9a2f9\n","  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=c27c66096f8db3755ac6881d1aee01e158dc365fd8b5aaa4230236470ab8061b\n","  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=4846cf78e304f0e262409c7fec0a0953285e93657639f9db9c94caf066a972db\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=e7c3b6f081a32bb941368ec6c1c0b2ae53f1a8407de5c34f840218b589618ec0\n","  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n","  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=b2fa39c7f26945e568429ab0e338f1e0dd4ae16cf9813927671e7415bac1709c\n","  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n","Successfully built gdown mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n","Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, py4j, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.11.4\n","    Uninstalling importlib-metadata-4.11.4:\n","      Successfully uninstalled importlib-metadata-4.11.4\n","  Attempting uninstall: more-itertools\n","    Found existing installation: more-itertools 8.13.0\n","    Uninstalling more-itertools-8.13.0:\n","      Successfully uninstalled more-itertools-8.13.0\n","  Attempting uninstall: hyperopt\n","    Found existing installation: hyperopt 0.1.2\n","    Uninstalling hyperopt-0.1.2:\n","      Successfully uninstalled hyperopt-0.1.2\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.4.0\n","    Uninstalling gdown-4.4.0:\n","      Successfully uninstalled gdown-4.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed bpemb-0.3.3 conllu-4.4.2 deprecated-1.2.13 flair-0.11.1 ftfy-6.1.1 gdown-3.12.2 huggingface-hub-0.8.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 pyyaml-6.0 requests-2.28.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.12.1 transformers-4.20.1 wikipedia-api-0.5.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b653eb74dcd190760295113ec8837a01df59d9f0199200a0dc14babdc96e5539\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.0.53\n"]}]},{"cell_type":"markdown","source":["#Tham khảo: https://github.com/flairNLP/flair"],"metadata":{"id":"YvTJIMH0P6wL"}},{"cell_type":"code","source":["from flair.data import Corpus\n","from flair.datasets import ColumnCorpus\n","from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n","from flair.models import SequenceTagger\n","from flair.trainers import ModelTrainer\n","\n","columns = {0: 'text',1:'pos',2:'pharse', 3: 'ner'}\n","\n","\n","# this is the folder in which train, test and dev files reside\n","data_folder = '/content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data'\n","\n","# init a corpus using column format, data folder and the names of the train, test files\n","##START CODE HERE##\n","corpus: Corpus = \n","##END CODE HERE##\n","label_type = 'ner'\n","\n","# 3. make the label dictionary from the corpus\n","label_dict = corpus.make_label_dictionary(label_type=label_type)\n","\n","# 4. initialize embedding stack with GloVe\n","##START CODE HERE##\n","\n","##END CODE HERE##\n","\n","\n","embeddings = StackedEmbeddings(embeddings=embedding_types)\n","\n","# 5. initialize sequence tagger \n","##START CODE HERE##\n","\n","##END CODE HERE##\n","# 6. initialize trainer\n","trainer = ModelTrainer(tagger, corpus)\n","\n","# 7. start training\n","trainer.train('content/model',\n","              learning_rate=0.1,\n","              mini_batch_size=4,\n","              max_epochs=10,\n","              )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvX_mvzLfDnQ","outputId":"330d0d38-98d6-418a-9418-abe2f8a79fcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:11,089 Reading data from /content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data\n","2022-06-23 20:19:11,091 Train: /content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data/train.conll\n","2022-06-23 20:19:11,094 Dev: None\n","2022-06-23 20:19:11,097 Test: /content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data/test.conll\n","2022-06-23 20:19:19,894 Computing label dictionary. Progress:\n"]},{"output_type":"stream","name":"stderr","text":["6989it [00:00, 48058.24it/s]"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:20,052 Dictionary created for label 'ner' with 5 values: PERSON (seen 2471 times), LOCATION (seen 2182 times), ORGANIZATION (seen 1433 times), DATETIME (seen 641 times)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:20,847 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmp4716tqgm\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 160000128/160000128 [00:09<00:00, 16549866.12B/s]"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:30,997 copying /tmp/tmp4716tqgm to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:31,231 removing temp file /tmp/tmp4716tqgm\n","2022-06-23 20:19:31,727 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmpm519w6_b\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21494764/21494764 [00:02<00:00, 9743855.09B/s] "]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:34,421 copying /tmp/tmpm519w6_b to cache at /root/.flair/embeddings/glove.gensim\n","2022-06-23 20:19:34,450 removing temp file /tmp/tmpm519w6_b\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:19:35,434 SequenceTagger predicts: Dictionary with 17 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-LOCATION, B-LOCATION, E-LOCATION, I-LOCATION, S-ORGANIZATION, B-ORGANIZATION, E-ORGANIZATION, I-ORGANIZATION, S-DATETIME, B-DATETIME, E-DATETIME, I-DATETIME\n","2022-06-23 20:19:50,122 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,124 Model: \"SequenceTagger(\n","  (embeddings): StackedEmbeddings(\n","    (list_embedding_0): WordEmbeddings(\n","      'glove'\n","      (embedding): Embedding(400001, 100)\n","    )\n","  )\n","  (word_dropout): WordDropout(p=0.05)\n","  (locked_dropout): LockedDropout(p=0.5)\n","  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n","  (rnn): GRU(100, 256, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=19, bias=True)\n","  (loss_function): ViterbiLoss()\n","  (crf): CRF()\n",")\"\n","2022-06-23 20:19:50,129 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,132 Corpus: \"Corpus: 6989 train + 776 dev + 1234 test sentences\"\n","2022-06-23 20:19:50,139 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,142 Parameters:\n","2022-06-23 20:19:50,144  - learning_rate: \"0.100000\"\n","2022-06-23 20:19:50,148  - mini_batch_size: \"4\"\n","2022-06-23 20:19:50,151  - patience: \"3\"\n","2022-06-23 20:19:50,152  - anneal_factor: \"0.5\"\n","2022-06-23 20:19:50,154  - max_epochs: \"10\"\n","2022-06-23 20:19:50,156  - shuffle: \"True\"\n","2022-06-23 20:19:50,157  - train_with_dev: \"False\"\n","2022-06-23 20:19:50,159  - batch_growth_annealing: \"False\"\n","2022-06-23 20:19:50,161 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,162 Model training base path: \"content/model\"\n","2022-06-23 20:19:50,164 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,166 Device: cuda:0\n","2022-06-23 20:19:50,167 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:50,169 Embeddings storage mode: cpu\n","2022-06-23 20:19:50,171 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:19:54,686 epoch 1 - iter 174/1748 - loss 0.29928835 - samples/sec: 154.50 - lr: 0.100000\n","2022-06-23 20:19:59,113 epoch 1 - iter 348/1748 - loss 0.27079749 - samples/sec: 157.60 - lr: 0.100000\n","2022-06-23 20:20:03,090 epoch 1 - iter 522/1748 - loss 0.26470007 - samples/sec: 175.48 - lr: 0.100000\n","2022-06-23 20:20:07,138 epoch 1 - iter 696/1748 - loss 0.25532224 - samples/sec: 172.28 - lr: 0.100000\n","2022-06-23 20:20:12,873 epoch 1 - iter 870/1748 - loss 0.24071936 - samples/sec: 121.60 - lr: 0.100000\n","2022-06-23 20:20:17,020 epoch 1 - iter 1044/1748 - loss 0.23377759 - samples/sec: 168.16 - lr: 0.100000\n","2022-06-23 20:20:21,122 epoch 1 - iter 1218/1748 - loss 0.22789026 - samples/sec: 170.01 - lr: 0.100000\n","2022-06-23 20:20:25,340 epoch 1 - iter 1392/1748 - loss 0.25026678 - samples/sec: 165.52 - lr: 0.100000\n","2022-06-23 20:20:29,981 epoch 1 - iter 1566/1748 - loss 0.28311660 - samples/sec: 150.26 - lr: 0.100000\n","2022-06-23 20:20:34,531 epoch 1 - iter 1740/1748 - loss 0.30514439 - samples/sec: 153.25 - lr: 0.100000\n","2022-06-23 20:20:34,742 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:20:34,745 EPOCH 1 done: loss 0.3058 - lr 0.100000\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 194/194 [00:04<00:00, 45.51it/s]"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:20:39,024 Evaluating as a multi-label problem: False\n","2022-06-23 20:20:39,042 DEV : loss 0.3749290704727173 - f1-score (micro avg)  0.2239\n","2022-06-23 20:20:39,099 BAD EPOCHS (no improvement): 0\n","2022-06-23 20:20:39,100 saving best model\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["2022-06-23 20:20:39,975 ----------------------------------------------------------------------------------------------------\n","2022-06-23 20:20:44,084 epoch 2 - iter 174/1748 - loss 0.27868244 - samples/sec: 169.87 - lr: 0.100000\n","2022-06-23 20:20:48,205 epoch 2 - iter 348/1748 - loss 0.27094325 - samples/sec: 169.26 - lr: 0.100000\n","2022-06-23 20:20:52,607 epoch 2 - iter 522/1748 - loss 0.26321948 - samples/sec: 158.48 - lr: 0.100000\n","2022-06-23 20:20:56,671 epoch 2 - iter 696/1748 - loss 0.26192402 - samples/sec: 171.78 - lr: 0.100000\n"]}]},{"cell_type":"markdown","source":["#Predict Model"],"metadata":{"id":"jEkHjPiHgteF"}},{"cell_type":"code","source":["from flair.models import SequenceTagger\n","from flair.data import Sentence\n","import json\n","\n","#Load Model\n","\n","\n","path_model = '/content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data/model/best-model.pt'\n","path_raw_text = '/content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data/data-thuc-the.txt'\n","path_write_file = '/content/drive/MyDrive/NLP/Đáp án cho bài tập thực hành/Lesson6/data/predict.json'\n","\n","# Load model\n","tagger = SequenceTagger.load(path_model)\n","\n","\n","f_raw_text = open(path_raw_text,'r',encoding='utf-8')\n","f_write = open(path_write_file,'w',encoding='utf-8')\n","lines = f_raw_text.readlines()\n","\n","# Tiến hành đọc các raw_text từ path_raw_text, sử dụng mô hình để in ra nhãn đoán, lưu file kết quả dưới dạng json \n","## START CODE HERE\n","\n","## END CODE HERE"],"metadata":{"id":"UEDUOzv9gvo3"},"execution_count":null,"outputs":[]}]}