{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"6r2DTPSyR_zy"},"source":["# Bài tập trên lớp về Spam Filtering\n","Trong bài tập này, chúng ta sẽ sử dụng các công cụ mạnh mẽ có sẵn như pandas/sklearn để thực hiện công việc phân biệt giữa mail spam và mail thường qua thông tin của chính email đó."]},{"cell_type":"markdown","metadata":{"id":"9qIsPctOR_z0"},"source":["## Download dữ liệu\n","Dữ liệu được tải về bằng gdown vào cùng folder với file hiện tại. Sau khi tải, chúng ta unzip dữ liệu nhận được."]},{"cell_type":"code","metadata":{"id":"M8bZn3J_R_z2","outputId":"b76054f7-a3e8-4c41-9514-6bfbd93d2452"},"source":["!gdown https://drive.google.com/uc?id=1bTJKchSInd3IgLs41b1_-Gd-T36a_pal -O spam_data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1bTJKchSInd3IgLs41b1_-Gd-T36a_pal\n","To: /home/khoai23/neural_network/jupyter/experimental/spam_data.zip\n","100%|██████████████████████████████████████| 1.95M/1.95M [00:00<00:00, 15.0MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TjqfoF6mR_0A","outputId":"dbbdf6a8-a73f-4e0c-ec63-5950bd1b78b5"},"source":["!unzip spam_data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  spam_data.zip\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u_N6amktR_0K"},"source":["## Sử dụng pandas để lưu trữ\n","Pandas là thư viện thường được sử dụng để cất giữ dữ liệu được sử dụng trong quá trình thực hiện các phương pháp học máy, với các chức năng phù hợp với dữ liệu lớn và hiệu năng cao. Chúng ta đọc dữ liệu của file đã được unzip vào một DataFrame."]},{"cell_type":"code","metadata":{"id":"tLhH97PbR_0M","outputId":"9a840e9e-fbcf-4278-ede8-8153058557fd"},"source":["import pandas as pd\n","data_loc = \"spam_ham_dataset.csv\"\n","\n","#Đọc dữ liệu sử dụng pandas\n","#YOUR CODE\n","\n","#END CODE\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>label_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>605</td>\n","      <td>ham</td>\n","      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2349</td>\n","      <td>ham</td>\n","      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3624</td>\n","      <td>ham</td>\n","      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4685</td>\n","      <td>spam</td>\n","      <td>Subject: photoshop , windows , office . cheap ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2030</td>\n","      <td>ham</td>\n","      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0 label                                               text  \\\n","0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n","1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n","2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n","3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n","4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n","\n","   label_num  \n","0          0  \n","1          0  \n","2          0  \n","3          1  \n","4          0  "]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"oVy0-zrNR_0V"},"source":["**Học viên in ra 5 mail spam và 5 mail ham đầu tiên xuất hiện trong dữ liệu ở block bên dưới:** "]},{"cell_type":"code","metadata":{"id":"w2ELZPU9R_0X"},"source":["# CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8NkhIv6R_0e"},"source":["## Data preprocessing\n","Như chúng ta có thể thấy, dữ liệu hiện tại đang chứa ký tự xuống dòng \\r\\n của Window và có thể dẫn đến ảnh hưởng xấu trong quá trình xây dựng chương trình. Để đơn giản hóa, chúng ta thay nó bằng dấu cách. Các bạn có thể ứng dụng phương án của pandas để thực hiện thêm các ý tưởng bản thân (v.d xóa chữ Subject: từ đầu, nhặt ra dòng đầu tiên, etc.)\n","\n","**Học viên sử dụng hàm `.apply` của pandas để format lại trường text, sử dụng hàm lambda tên `format_fn`:**"]},{"cell_type":"code","metadata":{"id":"vzq5ocFUR_0f"},"source":["format_fn = lambda x: # YOUR CODE HERE\n","spam_data[\"text\"] = spam_data[\"text\"].apply(format_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lower text"],"metadata":{"id":"X27NXOjBrSO7"}},{"cell_type":"code","source":["format_fn = lambda x: # YOUR CODE HERE\n","spam_data[\"text\"] = spam_data[\"text\"].apply(format_fn)"],"metadata":{"id":"5b4cVzU-rXVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afL5adZ1R_0l"},"source":["## Building Model\n","Đầu tiên, chúng ta thực hiện Vector hóa dữ liệu đầu vào qua CountVectorizer với mục tiêu là vector hóa dữ liệu từ. CountVectorizer có nhiệm vụ tạo một vocab các từ xuất hiện trong dữ liệu, và tạo một vector tương ứng cho mỗi sample là lần xuất hiện của các từ trong sample đó."]},{"cell_type":"code","metadata":{"id":"k6RXA3UrR_0m"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count = CountVectorizer(min_df=3, max_df=0.95)\n","\n","#Chuyển đổi dữ liệu và in ra shape của dữ liệu\n","#YOUR CODE\n","\n","#END CODE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0m34AjHcR_0u"},"source":["**Tiếp** đó, chúng ta áp dụng thuật toán TF-IDF lên vector đã tìm được. Dữ liệu được trả ra vẫn sẽ là một ma trận sparse, nhưng đã được adapt cho độ hiếm của mỗi từ."]},{"cell_type":"code","metadata":{"id":"bisAyplYR_0v"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","transformer = TfidfTransformer()\n","\n","#Trích rút đặc trưng TF-IDF và in ra hình dạng dữ liệu\n","#YOUR CODE\n","\n","#END CODE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHuS8dfKR_00"},"source":["Chúng ta có thể tạo dữ liệu dạng ngram qua argument `ngram_range` cho CountVectorizer, điều này cho phép chương trình lưu xuống các cụm n-từ thường thấy trong dữ liệu.\n","Ngoài ra, CountVectorizer và TfidfTransformer có một wrapper tổng hợp cả 2 quá trình vào 1 và nhận chung các argument của chúng: TfidfVectorizer.\n","\n","**Học viên đọc về class này và thực hiện count, TF-IDF và ứng dụng n-gram, đưa kết quả vào biến `matrix_data`**"]},{"cell_type":"code","metadata":{"id":"bs6-Z--8R_01"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#Sử dụng TfidfVectorizer\n","#YOUR CODE\n","\n","#END CODE\n","\n","print(matrix_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FDJXAr1R_06"},"source":["Sau khi hoàn thành các bước vector hóa, chúng ta tiếp tục phân hóa dữ liệu thành tập train và tập test để đánh giá chất lượng mô hình."]},{"cell_type":"code","metadata":{"id":"dGnVFMczR_07"},"source":["from sklearn.model_selection import train_test_split\n","X_data = matrix_data; y_data = spam_data[\"label_num\"].values\n","\n","#Chia dữ liệu thành các tập nhỏ với test_size = 0.3\n","X_train, X_test, y_train, y_test = #YOUR CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwOuceR_R_1A"},"source":["Sau khi chúng ta đã có dữ liệu đã vector hóa, chúng ta sử dụng một mô hình Bayes để tính kết quả. Yêu cầu trong tiết này là sử dụng mô hình dạng Naive Bayes (thuộc module `sklearn.naive_bayes`). Chúng ta có thể sử dụng nhiều mô hình học máy khác vào đây (e.g LogisticRegression, SVM).\n","\n","*Lưu ý: todense() được sử dụng để chuyển một ma trận sparse sang dense, có thể dẫn đến MemoryError với các dữ liệu lớn. Cân nhắc sử dụng các mô hình chấp nhận train qua ma trận sparse hoặc implement phương án của bản thân ở đây.*\n","\n","**Học viên import một mô hình trong module trên và thực hiện train bằng hàm `.fit`:**"]},{"cell_type":"code","metadata":{"id":"FjOifMX7R_1B","outputId":"93187a64-a9f9-4f3a-f9b0-02b3ded782a4"},"source":["# import your model here\n","model = # YOUR MODEL\n","\n","#Huấn luyện mô hình\n","#YOUR CODE\n","\n","#END CODE"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB(priors=None, var_smoothing=1e-09)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"YtxA6HVNR_1H"},"source":["#Dự đoán\n","y_pred = model.predict(X_test.todense())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwgxYdUPR_1M"},"source":["## Metrics and Visualization\n","Để đánh giá chất lượng mô hình, chúng ta có thể tính số điểm F1 hoặc accuracy. Trong trường hợp này F1 biểu diễn được chất lượng mô hình chính xác hơn, do độ phủ của class trong dữ liệu không giống nhau (25% là spam). Chúng ta cũng có thể in đường cong ROC-AUC để biểu thị các vị trí cutoff khác nhau cho mô hình"]},{"cell_type":"code","metadata":{"id":"85Oo7ZGVR_1N"},"source":["from sklearn.metrics import f1_score, accuracy_score\n","print(\"F1 Score: {:.4f}; Accuracy Score: {:.4f}\".format(f1_score(y_pred, y_test), accuracy_score(y_pred, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-5mAGjBR_1S"},"source":["from sklearn.metrics import roc_curve, auc\n","y_pred_proba = model.predict_proba(X_test.todense())\n","fpr_spam, tpr_spam, thresholds = roc_curve(y_test, y_pred_proba[:, 1], pos_label=1)\n","roc_auc_spam = auc(fpr_spam, tpr_spam)\n","fpr_ham, tpr_ham, thresholds = roc_curve(y_test, y_pred_proba[:, 0], pos_label=0)\n","roc_auc_ham = auc(fpr_spam, tpr_ham)\n","\n","import matplotlib.pyplot as plt\n","plt.figure()\n","lw = 2\n","plt.plot(fpr_spam, tpr_spam, color='darkorange',\n","         lw=lw, label='ROC curve (spam, area = %0.2f)' % roc_auc_spam)\n","plt.plot(fpr_ham, tpr_ham, color='red',\n","         lw=lw, label='ROC curve (ham, area = %0.2f)' % roc_auc_ham)\n","plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic example')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBHxEfRRR_1Y"},"source":["## Alternative: HashingVectorizer\n","Với các bộ dữ liệu lớn dẫn đến vocab cao, việc sử dụng CountVectorizer thông thường để lưu trữ dữ liệu vector hóa trở nên tốn tài nguyên; Một phương pháp chúng ta có thể cân nhắc là sử dụng HashingVectorizer. Thay vì CountVectorizer biến mỗi từ/ngram thành 1 id tương ứng, nhiều từ của HashingVectorizer sẽ có thể cho nhiều từ vào 1 id xác định bằng mảng băm.\n","\n","Lựa chọn giữa 2 phương pháp là tradeoff giữa tài nguyên lưu trữ như RAM và chất lượng mô hình. Thay đổi giá trị n_features và cân nhắc tradeoff ở bao nhiêu là phù hợp để mô hình không bị kém đi quá nhiều.\n","\n","**Học viên tìm và thử nghiệm giá trị `hash_size`, sao cho mô hình không chênh lệch quá lớn với kết quả gốc:**"]},{"cell_type":"code","metadata":{"id":"5J9y1LtxR_1Y"},"source":["from sklearn.feature_extraction.text import TfidfTransformer, HashingVectorizer\n","\n","hash_size = # YOUR VALUE HERE\n","hashed_data = HashingVectorizer(n_features=hash_size, ngram_range=(1, 3), stop_words='english').fit_transform(spam_data[\"text\"])\n","hashed_matrix_data = TfidfTransformer().fit_transform(hashed_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrdLq9uR_1b"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import f1_score\n","\n","#Chia dữ liệu\n","X_htrain, X_htest, y_htrain, y_htest = #YOUR CODE\n","model = GaussianNB()\n","model.fit(X_htrain.todense(), y_htrain)\n","y_hpred = model.predict(X_htest.todense())\n","print(\"[HashedVectorizer] F1 Score: {:.4f}; Accuracy Score: {:.4f}\".format(f1_score(y_hpred, y_htest), accuracy_score(y_hpred, y_htest)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDSFaf0lR_1f"},"source":[],"execution_count":null,"outputs":[]}]}