{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"les15_home_sol.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9235Ur6EDCzH"},"source":["# ELECTRA cho bài toán Text Classification\n","Trong notebook này, ta sẽ thực hành bài toán phân loại văn bản với mô hình ELECTRA\n","\n","Hiện tại, mô hình ELECTRA đã được đóng gói thành một module mà ta có thể gọi qua thư viện transformers. Điều này giúp cho việc thử nghiệm với mô hình trở nên dễ dàng hơn rất nhiều.\n","\n","**General Language Understanding Evaluation** (GLUE) là một bộ Benchmark được dùng để đánh giá khả năng đọc hiểu ngôn ngữ của các mô hình AI dành cho ngôn ngữ tiếng Anh. Bộ benchmark GLUE được tổng hợp từ 10 bài toán nhỏ khác nhau, đại diện cho những bài toán tiêu biểu trong xử lý ngôn ngữ. Trong đó, bài toán text classification được đại diện bởi dataset Stanford Sentiment Tree-2 (SST-2). Ta có thể đọc thêm về GLUE [tại đây](https://gluebenchmark.com/).\n","\n","Trong khuôn khổ của notebook này, ta sẽ thực hiện bài toán phân loại văn bản với bộ data SST-2 (đã được cung cấp)\n","\n","Sau khi hoàn thiện notebook này, học viên sẽ có:\n","- Hiểu biết sơ bộ về thư viện Pytorch dùng trong Deep Learning và Transformers cho các tác vụ xử lý ngôn ngữ\n","- Biết cách xây dựng một mô hình phân loại văn bản dùng ELECTRA\n"]},{"cell_type":"markdown","metadata":{"id":"qtFH9q_cZl4M"},"source":["## Bước 1: Thiết lập môi trường lập trình và cài đặt thư viện"]},{"cell_type":"code","metadata":{"id":"i1Db65yiMZRA","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600865778957,"user_tz":-420,"elapsed":26889,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"f627e35a-aff6-4852-ab06-c3cf0e6486bd"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YyaUzBPxZv3b"},"source":["Ta nên chuyển thư mục làm việc vào nơi chứa dataset của bộ SST-2 vào thư mục mà ta muốn làm việc."]},{"cell_type":"code","metadata":{"id":"mBlfsKGVMeWg"},"source":["import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/VinBDI-Daotao/Deep Learning/Question Answering')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8W25CQTzaOAS"},"source":["Sử dụng lệnh `!ls` để kiểm tra các file đã đầy đủ chưa."]},{"cell_type":"code","metadata":{"id":"OwFMW6nBN3Lg","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600865781376,"user_tz":-420,"elapsed":29285,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"abcc5f6d-a84a-4641-df44-f5ddcca30660"},"source":["!ls glue_data/SST-2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dev.tsv  original  test.tsv  train.tsv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rzRpbi4CN6j2"},"source":["Ngoài bộ SST-2, ta cũng có thể download các bộ dữ liệu của GLUE thông qua mã nguồn [tại đây](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)\n","\n","Sau khi đã có môi trường lập trình, ta cài đặt thư viện `transformers`"]},{"cell_type":"code","metadata":{"id":"Na3acv9KPdHv","colab":{"base_uri":"https://localhost:8080/","height":641},"executionInfo":{"status":"ok","timestamp":1600865787019,"user_tz":-420,"elapsed":34912,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"676ce9c2-2847-418c-eef9-6161760f974b"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/f4/9f93f06dd2c57c7cd7aa515ffbf9fcfd8a084b92285732289f4a5696dd91/transformers-3.2.0-py3-none-any.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 31.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 53.0MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 49.6MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=79596243d8e252010065f29d9b0daf629521185b32063213119ab979d55edd00\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oAzSkCvm2vCs"},"source":["## Bước 2: Làm quen với thư viện và dữ liệu"]},{"cell_type":"markdown","metadata":{"id":"OQhcZ5QcauY7"},"source":["### 2.1: Làm quen với thư viện `transformers`\n","\n","Trong thư viện này, ta sẽ được cung cấp API để sử dụng nhiều mô hình transformers khác nhau chỉ với một vài dòng lệnh.\n","\n","Để có thể sử dụng mô hình Electra cho bài toán phân loại văn bản, ta sẽ dùng ba class:\n","- transformers.ElectraConfig: để cấu hình cho mô hình ELECTRA\n","- transformers.ElectraTokenizer: để mã hóa văn bản thành các token để đưa vào mô hình Electra\n","- transformers.ElectraForSequenceClassification: module cung cấp cho chúng ta kiến trúc mô hình ELECTRA đi kèm với một tầng neuron để phân loại sau cùng.\n","\n","Ví dụ về cách sử dụng như sau:"]},{"cell_type":"code","metadata":{"id":"e_IYSLU-bA2H","colab":{"base_uri":"https://localhost:8080/","height":161},"executionInfo":{"status":"ok","timestamp":1600873619901,"user_tz":-420,"elapsed":2814,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"4b11e556-9988-4eea-f27f-eef7a706012c"},"source":["from transformers import ElectraTokenizer, ElectraForSequenceClassification, ElectraConfig\n","import torch\n","\n","#1. Khai báo hai đối tượng tokenizer với phiên bản small\n","tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n","config = ElectraConfig(num_labels =2)\n","model = ElectraForSequenceClassification(config).from_pretrained('google/electra-small-discriminator', return_dict=True)\n","\n","\n","#2. Chuyển đổi văn bản thành chuỗi các token và định nghĩa label cho mô hình\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n","\n","#3 Nạp đầu vào và đầu ra cho mô hình\n","outputs = model(**inputs, labels=labels)\n","\n","# Lấy ra giá trị loss và giá trị dự đoán của mô hình\n","loss = outputs.loss\n","logits = outputs.logits\n","\n","print(loss)\n","print(logits)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["tensor(0.7003, grad_fn=<NllLossBackward>)\n","tensor([[-0.0148, -0.0291]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LNkbmZnfg9tw"},"source":["Sau khi khởi tạo, object `model` sẽ có tính chất như một model thông thường của Pytorch. Cách huấn luyện model như thế nào ta sẽ cùng tìm hiểu trong notebook dưới đây."]},{"cell_type":"markdown","metadata":{"id":"ncJmmal8gkBT"},"source":["### Bước 2.2: Import và quan sát dữ liệu\n","\n","Các file dữ liệu của SST-2 được lưu dưới định dạng `.tsv`, chia thành ba subset là train, dev, test. Đầu tiên, hãy import 3 file dữ liệu vào 3 dataframe trong  notebook này. Ta có thể dùng `pd.read_csv()` để thực hiện điều đó. "]},{"cell_type":"code","metadata":{"id":"EZYcOM8aOapG"},"source":["import pandas as pd\n","import torch.optim as optim\n","from torch import nn\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification, ElectraConfig\n","import torch\n","import torch.optim as optim\n","from torch import nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2mpCsJiiOiF","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1600874169155,"user_tz":-420,"elapsed":1051,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"cb9c24cf-c1d3-484e-fe9b-4898ec9aaaa5"},"source":["# Dữ liệu training\n","# YOUR CODE HERE\n","train_df =  pd.read_csv(\"glue_data/SST-2/train.tsv\", sep='\\t', error_bad_lines=False)\n","print(train_df.shape)\n","train_df.head()\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(67349, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hide new secretions from the parental units</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>contains no wit , only labored gags</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>that loves its characters and communicates som...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>remains utterly satisfied to remain the same t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  label\n","0       hide new secretions from the parental units       0\n","1               contains no wit , only labored gags       0\n","2  that loves its characters and communicates som...      1\n","3  remains utterly satisfied to remain the same t...      0\n","4  on the worst revenge-of-the-nerds clichés the ...      0"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"hOiO9KtMR6h8","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1600865793508,"user_tz":-420,"elapsed":41370,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"99d6f3e2-7a4b-4510-ff74-166226377da1"},"source":["# Dữ liệu valuation\n","# YOUR CODE HERE\n","dev_df =  pd.read_csv(\"glue_data/SST-2/dev.tsv\", sep='\\t', error_bad_lines=False)\n","print(dev_df.shape)\n","dev_df.head()\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(872, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>it 's a charming and often affecting journey .</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>unflinchingly bleak and desperate</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>allows us to hope that nolan is poised to emba...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>the acting , costumes , music , cinematography...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>it 's slow -- very , very slow .</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  label\n","0    it 's a charming and often affecting journey .       1\n","1                 unflinchingly bleak and desperate       0\n","2  allows us to hope that nolan is poised to emba...      1\n","3  the acting , costumes , music , cinematography...      1\n","4                  it 's slow -- very , very slow .       0"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"fCltxanyO7ll","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1600865793511,"user_tz":-420,"elapsed":41358,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"fc40343d-1ad5-4635-a6fa-1ab4f3f67ccc"},"source":["# Dữ liệu test\n","# YOUR CODE HERE\n","test_df =  pd.read_csv(\"glue_data/SST-2/test.tsv\", sep='\\t', error_bad_lines=False)\n","print(test_df.shape)\n","test_df.head()\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1821, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>uneasy mishmash of styles and genres .</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>this film 's relationship to actual tension is...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>by the end of no such thing the audience , lik...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>director rob marshall went out gunning to make...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>lathan and diggs have considerable personal ch...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index                                           sentence\n","0      0             uneasy mishmash of styles and genres .\n","1      1  this film 's relationship to actual tension is...\n","2      2  by the end of no such thing the audience , lik...\n","3      3  director rob marshall went out gunning to make...\n","4      4  lathan and diggs have considerable personal ch..."]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"1eRAac7di7Cm"},"source":["Trong các file data mà ta được cung cấp, có 2 file có nhãn là file `train` và file `dev`. Ta sẽ dùng dữ liệu từ 2 file này trong mã nguồn của chúng ta: file train để train model và file dev để kiểm thử"]},{"cell_type":"markdown","metadata":{"id":"tQM5rkNrjh2D"},"source":["## Bước 3: Tiền xử lý và xây dựng mô hình"]},{"cell_type":"markdown","metadata":{"id":"bNvi4tT6jzJe"},"source":["### 3.1. Lập trình hàm `tokenize` để mã hóa văn bản thành các token\n","\n","Đầu vào là một hoặc một list các văn bản, đầu ra là object đã được biến đổi, đồng thời, hàm này sẽ cho ta lựa chọn giữa các phiên bản small, base, large của ELECTRA"]},{"cell_type":"code","metadata":{"id":"MgexBil0ksXr"},"source":["def tokenize(text, version = \"small\"):\n","  # YOUR CODE HERE\n","  tokenizer = ElectraTokenizer.from_pretrained('google/electra-{}-discriminator'.format(version))\n","  transformed_data = tokenizer(text, return_tensors=\"pt\", padding = True)\n","  return transformed_data\n","  # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAfSmSWnlIy8","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600875019223,"user_tz":-420,"elapsed":17258,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"ff7a89cc-2f12-4978-f7cb-41c8ab93ffa0"},"source":["#tokenize dữ liệu huấn luyện với hàm tokenize()\n","train_data = tokenize(train_df.sentence.values.tolist())\n","type(train_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.tokenization_utils_base.BatchEncoding"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"v900SqHvTEFh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600875049873,"user_tz":-420,"elapsed":1566,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"d7996609-c72b-4cbb-ad99-1c9018d1d0fb"},"source":["# tokenize dữ liệu validation với hàm tokenize()\n","dev_data = tokenize(dev_df.sentence.values.tolist())\n","type(dev_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.tokenization_utils_base.BatchEncoding"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"6DoR3uDhlyd0"},"source":["### 3.2. Lấy nhãn của train và dev data và chuyển đổi thành kiểu dữ liệu `torch.Tensor`"]},{"cell_type":"code","metadata":{"id":"YrV9ZG58lxhu","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600875189570,"user_tz":-420,"elapsed":1040,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"7aa354ba-564d-40eb-c52e-ab6970102d75"},"source":["# chuyển đổi nhãn của tập train thành Tensor sử dụng torch.Tensor()\n","train_labels = torch.Tensor(train_df.label)\n","type(train_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"LDYrDRieUPaV","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600875206161,"user_tz":-420,"elapsed":1065,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"541c7ce0-6225-44f2-9c88-c112d641eb4c"},"source":["# chuyển đổi nhãn của tập validation thành Tensor sử dụng torch.Tensor()\n","dev_labels = torch.Tensor(dev_df.label)\n","type(dev_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"SD_AnspfmWX5"},"source":["### 3.3. Lập trình hàm để huấn luyện mô hình\n","\n","Lập trình hàm `train` để khởi tạo và huấn luyện mô hình cho bài toán của notebook. Hàm train sẽ nhận vào những tham số như sau:\n","- `train_data`: dữ liệu để huấn luyện, thuộc kiểu `transformers.tokenization_utils_base.BatchEncoding`\n","- `batch_size`: Kích thước của dữ liệu huấn luyện,\n","- `epochs`: Số vòng huấn luyện\n","- `use_cuda`: Để là `True` nếu ta muốn dùng GPU để huấn luyện\n","- `display_every`: Hiển thị thông số training sau bao nhiêu bước\n","- `version`: Phiên bản ELECTRA mà ta muốn sử dụng\n","\n","Hàm train trong notebook này đã được viết một phần, nhiệm vụ của bạn là hoàn thành phần đọc dữ liệu theo từng batch để đưa vào mô hình huấn luyện.\n","\n","*Lưu ý*: Mô hình ElectraForSequenceClassification() sẽ nhận ba đầu vào là input_ids, token_type_ids và attention_mask (tương ứng với ba attribute cùng tên trong object `train_data`. Trong đó, input_ids là bắt buộc."]},{"cell_type":"code","metadata":{"id":"9k3q6KGbuW05","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600877351232,"user_tz":-420,"elapsed":904,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"0de61f08-933d-4670-dde0-6465af6eea10"},"source":["# 3 input của train_data mà ta sẽ phải đưa vào mô hình\n","train_data.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"On81yCsRat5p"},"source":["def train(train_data, batch_size, epochs, use_cuda = True, display_every = 200, version = \"small\"):\n","  # Lấy kích thước dữ liệu và khởi tạo mô hình\n","  DATA_SIZE = len(train_data.input_ids)\n","  model = ElectraForSequenceClassification.from_pretrained('google/electra-{}-discriminator'.format(version), return_dict=True)\n","  \n","  # Chuyển đổi mô hình sang chạy trên GPU nếu option use_cuda=True\n","  if use_cuda:\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","  \n","  # Định nghĩa hàm loss và thuật toán tối ưu cho mô hình\n","  criterion = nn.BCELoss()\n","  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","\n","  for epoch in range(2):  # loop over the dataset multiple times\n","\n","    train_batch = batch_size\n","    cursor = 0\n","    running_loss = 0.0\n","    \n","    # Đọc dữ liệu theo từng batch và đưa vào mô hình để huấn luyện\n","\n","    # YOUR CODE HERE\n","    while cursor < DATA_SIZE:\n","      step = cursor/train_batch + 1\n","      if cursor + train_batch>=DATA_SIZE:\n","        train_batch = DATA_SIZE - cursor\n","\n","      # get the inputs; data is a list of [inputs, labels]\n","      input_id = train_data.input_ids[cursor:cursor+train_batch].type(torch.LongTensor)\n","      token_id = train_data.token_type_ids[cursor:cursor+train_batch].type(torch.LongTensor)\n","      attn_mask = train_data.attention_mask[cursor:cursor+train_batch].type(torch.LongTensor)\n","      label = train_labels[cursor:cursor+train_batch].type(torch.LongTensor)\n","        \n","    # YOUR CODE HERE\n","\n","\n","      # Chuyển dữ liệu sang chế độ GPU nếu có\n","      # YOUR CODE HERE\n","      if use_cuda:\n","          input_id = input_id.cuda()\n","          token_id = token_id.cuda()\n","          attn_mask = attn_mask.cuda()\n","          label = label.cuda()\n","\n","      # YOUR CODE HERE\n","\n","      # zero the parameter gradients\n","      optimizer.zero_grad()\n","\n","\n","      # Tiếp theo, ta viết phần để mô hình thực hiện ba phần: forward_propagation, back_propagation, optimzation\n","\n","      # YOUR CODE HERE\n","      # Giai đoạn forward propagation (truyền xuôi) của mô hình\n","      outputs = model(input_ids = input_id, token_type_ids = token_id, attention_mask = attn_mask, labels= label)\n","      \n","      # Minimize hàm loss qua câu lệnh loss.backward()\n","\n","      loss = outputs.loss\n","      loss.backward()\n","      \n","      # Tối ưu tham số mô hình  qua hàm optimizer.step()\n","      optimizer.step()\n","\n","      # YOUR CODE HERE\n","      # Dịch chuyển sang batch tiếp theo để huấn luyện\n","      cursor += train_batch\n","\n","      # In thông số huấn luyện\n","      running_loss += loss.item()\n","      if step%display_every == 0:\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, step, running_loss /(display_every*train_batch)))\n","            running_loss = 0.0\n","\n","  print('Finished Training')\n","\n","  return model    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjAjpEv9uwM5"},"source":["### 3.4. Huấn luyện mô hình"]},{"cell_type":"code","metadata":{"id":"uZvzYK0-c7i_","colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"status":"ok","timestamp":1600877132794,"user_tz":-420,"elapsed":259162,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"04cd14a4-d3d7-446f-e561-7e404b69b7ce"},"source":["model = train(train_data, 32, 2, use_cuda = True, display_every = 200, version = \"small\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["[1,   200] loss: 0.021\n","[1,   400] loss: 0.014\n","[1,   600] loss: 0.011\n","[1,   800] loss: 0.009\n","[1,  1000] loss: 0.009\n","[1,  1200] loss: 0.008\n","[1,  1400] loss: 0.008\n","[1,  1600] loss: 0.007\n","[1,  1800] loss: 0.007\n","[1,  2000] loss: 0.007\n","[2,   200] loss: 0.007\n","[2,   400] loss: 0.006\n","[2,   600] loss: 0.006\n","[2,   800] loss: 0.005\n","[2,  1000] loss: 0.005\n","[2,  1200] loss: 0.005\n","[2,  1400] loss: 0.005\n","[2,  1600] loss: 0.004\n","[2,  1800] loss: 0.005\n","[2,  2000] loss: 0.005\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SwxfmctXNARl"},"source":["#Sau khi training, ta có thể lưu mô hình vào một file `.pth` để dùng lại khi cần\n","PATH = './ELECTRA_net.pth'\n","torch.save(model.state_dict(), PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gwf55T7Musns"},"source":["### 3.5 Đánh giá mô hình trên tập kiểm thử\n","Dưới đây ta có một hàm để đánh giá hiệu năng của mô hình, hãy hoàn thành phần so sánh giữa nhãn dự đoán và nhãn thực của code để hoàn thiện hàm.\n"]},{"cell_type":"code","metadata":{"id":"sJZ_UGWEfocb"},"source":["def evaluate(dev_data, model,  batch_size = 32, use_cuda = True):\n","  test_size = len(dev_data.input_ids)\n","  correct = 0\n","  total = 0\n","  with torch.no_grad():\n","    cursor = 0\n","    running_loss = 0.0\n","    while cursor < test_size:\n","      step = cursor/batch_size + 1\n","      if cursor + batch_size>=test_size:\n","        batch_size = test_size - cursor\n","\n","      # get the inputs; data is a list of [inputs, labels]\n","      input_id = dev_data.input_ids[cursor:cursor+batch_size].type(torch.LongTensor)\n","      token_id = dev_data.token_type_ids[cursor:cursor+batch_size].type(torch.LongTensor)\n","      attn_mask = dev_data.attention_mask[cursor:cursor+batch_size].type(torch.LongTensor)\n","      label = dev_labels[cursor:cursor+batch_size].type(torch.LongTensor)\n","        \n","      if use_cuda:\n","          input_id = input_id.cuda()\n","          token_id = token_id.cuda()\n","          attn_mask = attn_mask.cuda()\n","          label = label.cuda()\n","\n","      outputs = model(input_ids = input_id, token_type_ids = token_id, attention_mask = attn_mask, labels= label)\n","      _, predicted = torch.max(outputs.logits, 1)\n","      \n","      total = len(dev_labels)\n","      \n","      # Hoàn thiện phần thống kê số nhãn dự đoán giống nhãn thực tế\n","      # YOUR CODE HERE\n","      correct += (predicted == label.cuda()).sum().item()\n","      # YOUR CODE HERE\n","\n","      cursor += batch_size\n","  print('Accuracy of the network on data: %d %%' % (100 * correct / total))\n","\n","  return 100 * correct / total\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHNzJP5emonC","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600878703761,"user_tz":-420,"elapsed":1235,"user":{"displayName":"Minh hiếu Phạm","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdTO3sbyLOUtyJOgzfOEdc8dLpJKSEze_ucPL1=s64","userId":"13621231777531886646"}},"outputId":"f5b25d00-390a-4a2c-9833-35bf1a0bd8b3"},"source":["evaluate(dev_data, model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on data: 89 %\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["89.79357798165138"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"X23Y1i0Y2bSi"},"source":["Với ELECTRA Small, ta có thể đạt độ chính xác 89% trên tập dev. Bạn có thể thử tăng cao độ chính xác bằng cách thay đổi tham số mô hình hoặc sử dụng phiên bản khác của ELECTRA.\n","\n","Nếu bạn đã làm đến đây, xin chúc mừng! Bây giờ bạn đã có trong tay kiến thức về:\n","- Một thư viện học sâu phổ biến hàng đầu thế giới\n","- Một thư viện nổi tiếng trong NLP\n","- Và trên hết, cách sử dụng một mô hình State-of-the-art trong Deep Learning.\n","\n","That's quite an achievement! Hy vọng những kiến thức bạn đã tích lũy được sẽ giúp ích cho bạn trong tương lai!"]}]}